# **6. 규제 선형 모델 - 릿지, 라쏘, 엘라스틱넷**

### **6-1. 규제 선형 모델의 개요**
- 회귀 모델은 적절히 데이터에 적합하면서도 회쉬 계수가 기하급수적으로 커지는 것을 제어할 수 있어야 함
- 비용 함수는 학습 데이터의 잔차 오류 값을 최소로 하는 RSS 최소화 방법과 과적합을 방지하기 위해 회귀 계수 값이 커지지 않도록 하는 방법이 균형을 이뤄야 함
	- 비용 함수 목표: MIN($RSS(W) + alpha * $)
- alpha: 학습 데이터 적합 정도와 회귀 계수 값의 크기 제어를 수행하는 튜닝 파라미터
	- alpha 값을 크게 하면 비용 함수는 회귀 계수 w의 값을 작게 해 과적합을 개선할 수 있으며 alpha 값을 작게 하면 회귀 계수 w의 값이 커져도 어느 정도 상쇄가 가능하므로 학습 데이터 적합을 더 개선할 수 있음
- 규제(Regularization): 비용 함수에 alpha 값으로 페널티를 부여해 회귀 계수 값의 크기를 감소시켜 과적합을 개선하는 방식
	- L2 방식: alpha * |W|와 같이 W의 재곱에 대해 페널티를 부여하는 방식, 릿지(Ridge)
	- L1 방식: alpha * |w|와 같이 w의 절댓값에 대해 패널티를 부여하는 방식, 라쏘(Lasso)
		- L1 규제 적용 시 영향력이 크지 않은 회귀 계수는 0이 됨 

### **6-2. 릿지 회귀**
- sklearn.Ridge 클래스를 통해 구현
- 주요 파라미터
	- alpha: L2 규제 계수


# **7. 로지스틱 회귀**
- 선형 회귀 방식을 **분류**에 적용한 알고리즘
  - 이름은 회귀지만 사실은 **분류** 알고리즘
- 회귀가 선형인가 비선형인가는 독립변수가 아닌 **가중치 변수(weight)** 가 선형인지 아닌지를 따름
  - 학습을 통해 선형 함수의 회귀 최적선을 찾는 것이 아니라 ```시그모이드(sigmoid)``` 함수 최적선을 찾고, 이 시그모이드 함수의 반환 값을 확률로 간주해 확률에 따라 분류를 결정하는 것 
- 선형 회귀 방식을 기반으로 하되 시그모이드 함수를 이용해 **분류**를 수행하는 회귀 방식
- 가볍고 빠름
- **이진** 분류 예측 성능이 뛰어나고, 희소한 데이터 세트 분류에도 뛰어난 성능을 보임

<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/d53ea33b-8e8c-44c5-ade1-d758f7d25d68" width = 400 height = 200>

- ```시그모이드 함수```
  - S자 커브 형태를 가짐
  - 정의) $y = \frac{1} {1+{e}^{-x}}$ 
  - y값은 항상 0과 1 사이 값을 반환함
  
- 주요 파라미터
  - ```penalty```
    - 규제(regularization)의 유형 설정 
    - 'l2'로 설정 시 L2 규제, 'l1'로 설정 시 L1 규제를 적용
    - default: 'l2'
  - ```C```
    - 규제 강도를 조절하는 alpha 값의 역수
    - 작을수록 규제 강도가 큼
    




















