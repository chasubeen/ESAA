# **1. 회귀 소개**
### **📌 회귀(regression)**
- 데이터 값이 평균과 같은 일정한 값으로 **돌아가려는** 경향을 이용한 통계학 기법
- 여러 개의 ```독립변수(X)```와 한 개의 ```종속변수(y)``` 간의 상관관계를 모델링하는 기법을 통칭
  - ML 관점에서 독립변수는 **피처**, 종속변수는 **결정 값**에 해당 
- ```회귀 계수(intercept)```: W1, W2, W3, ... / 독립변수의 값에 영향을 미치는 것들
- 머신러닝 회귀 예측의 핵심은 주어진 피처와 결정 값 데이터 기반에서 학습을 통해 **최적의 회귀 계수**를 찾아내는 것
- 회귀는 회귀 계수의 선형/비선형 여부, 독립변수의 개수, 종속변수의 개수에 따라 여러 가지 유형으로 나눌 수 있음
  - 회귀 계수의 선형 여부에 따라 **선형 회귀**와 **비선형 회귀**로 나눌 수 있음
  - 독립변수의 개수에 따라 **단일 회귀**와 **다중 회귀**로 나눌 수 있음  

### **📌 선형 회귀**
- 여러 가지 회귀 중에서 선형 회귀가 가장 많이 사용됨
- 선형 회귀는 실제 값과 예측값의 **차이**(오류의 제곱값)를 최소화하는 직선형 회귀선을 최적화하는 방식
- **규제(Regularization)** 방법에 따라 다시 별도의 유형으로 나눌 수 있음
  - 규제: 일반적인 선형 회귀의 과적합 문제를 해결하기 위해서 회귀 계수에 패널티 값을 적용하는 것
- **대표적인 선형 회귀 모델**  
  - ```일반 선형 회귀```
    - 예측값과 실제 값의 RSS(Residual Sum of Squares)를 최소화 할 수 있도록 회귀 계수를 최적화
    - 규제 적용 x
  - ```릿지(Ridge)```
    - 선형 회귀에 L2 규제를 추가한 회귀 모델
    - L2 규제를 적용
    - L2 규제: 상대적으로 큰 회귀 계수 값의 예측 영향도를 감소시키기 위해 회귀 계수값을 더 작게 만드는 규제 모델
  - ```라쏘(Lasso)```
    - 선형 회귀에 L1 규제를 적용한 방식
    - L1 규제: 예측 영향력이 작은 피처의 회귀 계수를 0으로 만들어 회귀 예측 시 피처가 선택되지 않게 하는 방식
  - ```엘라스틱넷(ElasticNet)```
    - L2, L1 규제를 함께 결합한 모델
    - 주로 피처가 많은 데이터 세트에서 적용됨
    - L1 규제로 피처의 개수를 줄임과 동시에 L2 규제로 계수 값의 크기를 조정함
  - ```로지스틱 회귀(Logistic Regression)```
    - 사실은 분류 모델
    - 일반적으로 이진 분류뿐만 아니라 최소 영역의 분류 등에서도 뛰어난 예측 성능을 보임  

# **2. 단순 선형 회귀를 통한 회귀 위해**
- 독립변수도 하나, 종속변수도 하나인 선형 회귀

<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/a7912b47-0e08-48ca-a844-bd8c1220d02c" width = 400 height = 300>

- 최적 회귀 모델은 **잔차**(실제 값 - 예측값)를 최소화하는 모델, 즉 오류값의 합이 최소가 되는 **최적의 회귀 계수**를 찾는 것을 의미
  - 오류 값은 양수/음수 둘 다 가능 => 주로 절댓값을 취해 더하거나(**MSE**), 오류 값의 제곱을 구해서 더하는 방식(**RSS**)을 택함
  - 일반적으로 미분 등의 계산을 편리하게 하기 위해서 **RSS** 방식을 택함
  
<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/b124f81e-39da-4773-b3f2-3c1c53d22daa" width = 400 height = 200>    

- 머신러닝 기반 회귀는 최적의 회귀 계수를 학습을 통해서 찾는 것이 핵심임
  - RSS는 회귀식의 독립변수 X, 종속변수 Y가 중심 변수가 아니라 **w 변수(회귀 계수)** 가 중심 변수임
  - 일반적으로 RSS는 학습 데이터의 건수로 나누어서 다음과 같이 정규화된 식으로 표현됨
  $$RSS(w_{0},w_{1}) = \frac{1}{N}\sum_1^N (y_{i}-(w_{0}+w_{1}*x_{i}))^2$$
  
  - 회귀에서 RSS는 **비용(cost)** 이며 w 변수(회귀 계수)로 구성됨
    - **비용 함수**를 지속해서 감소시키고 최종적으로는 더 이상 감소하지 않는 최소의 오류 값을 구하는 것이 핵심

# **3. 비용 최소화하기**
### **3-1. 경사 하강법(Gradient Descent)**
- 경사 하강법은 고차원 방정식에 대한 문제를 해결해 주면서 비용 함수 RSS를 **최소화**하는 방법을 직관적으로 제공하는 뛰어난 방식임
- **점진적으로** 반복적인 계산을 통해 W 파라미터 값을 업데이트하면서 오류값이 최소가 되는 W 파라미터를 구하는 방식
- W 파라미터의 개수에 따라 매우 복잡해지는 고차원 방정식을 푸는 것보다 훨씬 더 직관적으로 빠르게 비용 함수가 최소가 되는 W 파라미터 값을 구할 수 있음

<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/25cc462f-b07a-4935-a024-5a3dcb5f29e9" width = 400 height = 250>

- 경사 하강법의 일반적인 프로세스
  - Step 1) 초기 가중치 w를 임의의 값으로 설정하고 첫 비용 함수의 값을 계산
  - Step 2) 학습을 통해 가중치를 업데이트 한 후 다시 비용 함수의 값을 계산
  - Step 3) 비용 함수의 값이 감소했다면 다시 Step 2)를 반복, 더 이상 비용 함수의 값이 감소하지 않으면 그때의 w를 구하고 반복 중지

- 예측 배열 y_pred는 ```np.dot(X, w1.T) + w0```임
- 100개의 데이터 X(1,2,...,100)이 있다면 예측값은 $w0 + X(1)w1 + X(2)w1 +⋅⋅⋅+ X(100)w1$ 이며, 이는 입력 배열 X와 w1 배열의 **내적**
  - 새로운 w1과 w0를 update함

- 일반적으로 경사 하강법은 **모든** 학습 데이터에 대해 반복적으로 비용함수 최소화를 위한 값을 업데이트
  - 수행 시간이 매우 오래 걸린다는 단점 존재
  - 실전에서는 대부분 **확률적** 경사 하강법을 이용

### **3-2. 확률적 경사 하강법(Stochastic Gradient Descent)**
- 전체 입력 데이터가 아닌 **일부** 데이터만 이용해 w가 업데이트되는 값을 계산함
  - 전체 X, y 데이터에서 랜덤하게 ```batch_size```만큼 데이터를 추출해 이를 기반으로 가중치를 업데이트 함 
  - 경사 하강법에 비해서 **빠른** 속도를 보장
 
### **3-3. 다항 회귀에서의 경사 하강법**
- 선형 대수를 이용하여 간단히 계산 가능

<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/72c5c550-0f2a-46ef-a9ec-f5167aceb362" width = 500 height = 250>

- w0을 weight 배열 내에 포함시키려면 다음과 같이 Xmat의 맨 처음 열에 feat 0(intercept)을 추가할 수 있음

<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/38f05184-efd3-4b93-9b35-981763455db8" width = 500 height = 300>









