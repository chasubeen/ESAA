# **1. 회귀 소개**
### **📌 회귀(regression)**
- 데이터 값이 평균과 같은 일정한 값으로 **돌아가려는** 경향을 이용한 통계학 기법
- 여러 개의 ```독립변수(X)```와 한 개의 ```종속변수(y)``` 간의 상관관계를 모델링하는 기법을 통칭
  - ML 관점에서 독립변수는 **피처**, 종속변수는 **결정 값**에 해당 
- ```회귀 계수(intercept)```: W1, W2, W3, ... / 독립변수의 값에 영향을 미치는 것들
- 머신러닝 회귀 예측의 핵심은 주어진 피처와 결정 값 데이터 기반에서 학습을 통해 **최적의 회귀 계수**를 찾아내는 것
- 회귀는 회귀 계수의 선형/비선형 여부, 독립변수의 개수, 종속변수의 개수에 따라 여러 가지 유형으로 나눌 수 있음
  - 회귀 계수의 선형 여부에 따라 **선형 회귀**와 **비선형 회귀**로 나눌 수 있음
  - 독립변수의 개수에 따라 **단일 회귀**와 **다중 회귀**로 나눌 수 있음  

### **📌 선형 회귀**
- 여러 가지 회귀 중에서 선형 회귀가 가장 많이 사용됨
- 선형 회귀는 실제 값과 예측값의 **차이**(오류의 제곱값)를 최소화하는 직선형 회귀선을 최적화하는 방식
- **규제(Regularization)** 방법에 따라 다시 별도의 유형으로 나눌 수 있음
  - 규제: 일반적인 선형 회귀의 과적합 문제를 해결하기 위해서 회귀 계수에 패널티 값을 적용하는 것
- **대표적인 선형 회귀 모델**  
  - ```일반 선형 회귀```
    - 예측값과 실제 값의 RSS(Residual Sum of Squares)를 최소화 할 수 있도록 회귀 계수를 최적
    - 규제 적용 x
  - ```릿지(Ridge)```
    - 선형 회귀에 L2 규제를 추가한 회귀 모델
    - L2 규제를 적용
    - L2 규제: 상대적으로 큰 회귀 계수 값의 예측 영향도를 감소시키기 위해 회귀 계수값을 더 작게 만드는 규제 모델
  - ```라쏘(Lasso)```
    - 선형 회귀에 L1 규제를 적용한 방식
    - L1 규제: 예측 영향력이 작은 피처의 회귀 계수를 0으로 만들어 회귀 예측 시 피처가 선택되지 않게 하는 방식
  - ```엘라스틱넷(ElasticNet)```
    - L2, L1 규제를 함께 결합한 모델
    - 주로 피처가 많은 데이터 세트에서 적용됨
    - L1 규제로 피처의 개수를 줄임과 동시에 L2 규제로 계수 값의 크기를 조정함
  - ```로지스틱 회귀(Logistic Regression)```
    - 사실은 분류 모델
    - 일반적으로 이진 분류뿐만 아니라 최소 영역의 분류 등에서도 뛰어난 예측 성능을 보임  

# **2. 단순 선형 회귀**
- 독립변수도 하나, 종속변수도 하나인 선형 회귀

<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/a7912b47-0e08-48ca-a844-bd8c1220d02c" width = 400 height = 300>

- 최적 회귀 모델은 **잔차**(실제 값 - 예측값)를 최소화하는 모델, 즉 오류값의 합이 최소가 되는 **최적의 회귀 계수**를 찾는 것을 의미
  - 오류 값은 양수/음수 둘 다 가능 => 주로 절댓값을 취해 더하거나(**MSE**), 오류 값의 제곱을 구해서 더하는 방식(**RSS**)을 택함
  - 일반적으로 미분 등의 계산을 편리하게 하기 위해서 **RSS** 방식을 택함
  
<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/b124f81e-39da-4773-b3f2-3c1c53d22daa" width = 400 height = 200>    

- 머신러닝 기반 회귀는 최적의 회귀 계수를 학습을 통해서 찾는 것이 핵심임
  - RSS는 회귀식의 독립변수 X, 종속변수 Y가 중심 변수가 아니라 **w 변수(회귀 계수)** 가 중심 변수임
  - 일반적으로 RSS는 학습 데이터의 건수로 나누어서 다음과 같이 정규화된 식으로 표현됨
  $$RSS(w_{0},w_{1}) = \frac{1}{N}\sum_1^N (y_{i}-(w_{0}+w_{1}*x_{i}))^2$$
  
  - 회귀에서 RSS는 **비용(cost)** 이며 w 변수(회귀 계수)로 구성됨
    - **비용 함수**를 지속해서 감소시키고 최종적으로는 더 이상 감소하지 않는 최소의 오류 값을 구하는 것이 핵심

# **3. 비용 최소화하기**
### **3-1. 경사 하강법(Gradient Descent)**
- 경사 하강법은 고차원 방정식에 대한 문제를 해결해 주면서 비용 함수 RSS를 **최소화**하는 방법을 직관적으로 제공하는 뛰어난 방식임
- **점진적으로** 반복적인 계산을 통해 W 파라미터 값을 업데이트하면서 오류값이 최소가 되는 W 파라미터를 구하는 방식
- W 파라미터의 개수에 따라 매우 복잡해지는 고차원 방정식을 푸는 것보다 훨씬 더 직관적으로 빠르게 비용 함수가 최소가 되는 W 파라미터 값을 구할 수 있음

<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/25cc462f-b07a-4935-a024-5a3dcb5f29e9" width = 400 height = 250>

- 경사 하강법의 일반적인 프로세스
  - Step 1) 초기 가중치 w를 임의의 값으로 설정하고 첫 비용 함수의 값을 계산
  - Step 2) 학습을 통해 가중치를 업데이트 한 후 다시 비용 함수의 값을 계산
  - Step 3) 비용 함수의 값이 감소했다면 다시 Step 2)를 반복, 더 이상 비용 함수의 값이 감소하지 않으면 그때의 w를 구하고 반복 중지

- 예측 배열 y_pred는 ```np.dot(X, w1.T) + w0```임
- 100개의 데이터 X(1,2,...,100)이 있다면 예측값은 $w0 + X(1)w1 + X(2)w1 +⋅⋅⋅+ X(100)w1$ 이며, 이는 입력 배열 X와 w1 배열의 **내적**
  - 새로운 w1과 w0를 update함

- 일반적으로 경사 하강법은 **모든** 학습 데이터에 대해 반복적으로 비용함수 최소화를 위한 값을 업데이트
  - 수행 시간이 매우 오래 걸린다는 단점 존재
  - 실전에서는 대부분 **확률적** 경사 하강법을 이용

### **3-2. 확률적 경사 하강법(Stochastic Gradient Descent)**
- 전체 입력 데이터가 아닌 **일부** 데이터만 이용해 w가 업데이트되는 값을 계산함
  - 전체 X, y 데이터에서 랜덤하게 ```batch_size```만큼 데이터를 추출해 이를 기반으로 가중치를 업데이트 함 
  - 경사 하강법에 비해서 **빠른** 속도를 보장
 
### **3-3. 다중 회귀에서의 경사 하강법**
- 선형 대수를 이용하여 간단히 계산 가능

<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/72c5c550-0f2a-46ef-a9ec-f5167aceb362" width = 500 height = 250>

- w0을 weight 배열 내에 포함시키려면 다음과 같이 Xmat의 맨 처음 열에 feat 0(intercept)을 추가할 수 있음

<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/38f05184-efd3-4b93-9b35-981763455db8" width = 500 height = 300>

# **4. 선형 회귀(Linear Regression)**
- 사이킷런의 ```linear_models``` 모듈은 매우 다양한 종류의 선형 기반 회귀를 클래스로 구현해 제공
- ```LinearRegression``` 클래스의 경우 규제가 적용되지 않은 회귀 모형
- [Reference](https://scikit-learn.org/stable/modules/linear_model.html)

### **4-1. LinearRegression 클래스 - Ordinary Least Squares**
- 예측값과 실제 값의 RSS(Residual Sum of Squares)를 최소하해 **OLS(Ordinary Least Squares)** 추정 방식으로 구현한 클래스

```Python
class sklearn.linear_model.LinearRegression(*, fit_intercept=True, 
                                            copy_X=True, n_jobs=None, positive=False)
```

- 입력 파라미터
  - ```fit_intercept```
    - 불린 값으로, 디폴트는 True 
    -  intercept(절편) 값을 계산할 것인지 말지를 지정
    -  만일 False로 지정하면 intercept가 사용되지 않고 0으로 지정됨
  - ```normalize```
    - 불린 값으로, 디폴트는 False
    - fit_intercept가 False인 경우에는 해당 파라미터가 무시됨
    - 만일 True이면 회귀를 수행하기 전에 입력 데이터 세트를 정규화 함
- 속성
  - ```coef_```: fit() 메서드를 수행했을 때 회귀 계수가 배열 형태로 저장됨
  - ```intercept_```: intercept 값

- Ordinary Least Sqaures 기반의 회귀 계수 계산은 입력 피처의 독립성에 많은 영향을 받음
  - 피처 간의 상관관계가 매우 높은 경우 분산이 매우 커져 오류에 매우 민감해짐
    - 다중공선성(multicollinearity) 문제
  - 일반적으로 상관관계가 높은 피처가 많은 경우 독립적인 중요한 피처만 남기고 제거하거나 규제를 적용함
    - PCA를 통한 차원 축소도 고려해 볼 수 있음

### **4-2. 회귀 평가 지표**
- 실제 값과 회귀 예측값의 차이 기반
<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/da0bdf2a-38d4-4dfe-ac52-2c4d8f357ec5" width = 600 height = 400>

-  
  - 이외에도 MSE나 RMSE에 로그를 적용한 MSLE와 RMSLE도 활용

- 각 평가 방법에 대한 사이킷런의 API 및 cross_val_score나 GridSearchCV에서 평가 시 사용되는 scoring 파라미터의 적용값

|평가 방법|사이킷런 평가 지표 API|Scoring 함수 적용 값|
|-----|----------|----------|
|MAE|metrics.mean_absolute_error|'neg_mean_absolute_error'|
|MSE|metrics.mean_squared_error|'neg_mean_squared_error'|
|R^2|metrics.r2_score|'r2'|

-  
  - 사이킷런의 Scoring 함수는 score 값이 무조건 클수록 좋은 평가 결과로 자동 평가함
    - 하지만 실제 값과 예측값의 오류 차이를 기반으로 하는 회귀 평가 지표의 경우 값이 커지면 오히려 나쁜 모델이라는 의미이므로 이를 사이킷런의 Scoring 함수에 일반적으로 반영하기 위해 **음수**로 보정함
    - -1을 원래의 평가 지표 값에 곱해서 **음수**를 만들어 작은 오류 값이 더 큰 숫자로 인식되도록 함
     
# **5. 다항 회귀와 과대/과소 적합**
### **5-1. 다항 회귀 이해**
- ```다항(Polynomial) 회귀```: 회귀가 독립변수의 단항식이 아닌 2차, 3차 방정식과 같은 다항식으로 표현되는 것
$y = w_{0} + w_{1}*x_{1} + w_{2}*x_{2} + w_{3}*x_{1}*x_{2} + w_{4}*x_{1}^2 + w_{5}*x_{2}^2$
- 다항 회귀는 **선형 회귀**임!
  - 회귀에서 선형/비선형 여부를 나누는 기준은 회귀 **계수**가 선형/비선형인지에 따른 것임
  - 독립 변수와는 무관함

- 사이킷런은 다항 회귀를 위한 클래스를 따로 제공하지는 않음
  - 대신 다항 회귀 역시 **선형** 회귀임 ->  비선형 함수를 선형 모델에 적용시키는 방법을 사용하여 구현
  - ```sklearn.PolynomialFeatures``` 클래스를 통해 피처를 다항 피처로 변환
    - ```degree``` 파라미터를 통해 입력 받은 단항식 피처를 degree에 해당하는 다항식 피처로 변환
  - 이후 ```sklearn.LinearRegression()``` 클래스로 다항 회귀 구현

- ```sklearn.Pipeline```을 통해 피처 변환과 선형 회귀 적용을 **한번에** 수행할 수 있음

### **5-2. 다항 회귀를 이용한 과소적합/과적합 이해**
- 다항 회귀는 피처의 직선적 관계가 아닌 복잡한 다항 관계를 모델링할 수 있음
- 다항식의 차수가 높아질수록 매우 복잡한 피처 간의 관계까지 모델링 가능
- 하지만 다항 회귀의 차수(degree)를 높일수록 학습 데이터에만 너무 맞춘 학습이 이뤄져 정작 테스트 데이터 환경에서는 오히려 예측 정확도가 떨어질 수 있음
  - 차수가 높아질수록 과적합 문제가 크게 발생함 
#### **📌 차수에 따른 예측 결과 비교**  
<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/818b473d-400b-40af-afd4-15100bdb03d1" width = 600 height = 300>

<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/284f2b54-7630-4f54-81e1-b6c891e39e6a" width = 600 height = 300>

> 좋은 예측 모델은 학습 데이터의 패턴을 지나치게 단순화한 과소적합 모델도 아니고 모든 학습 데이터의 패턴을 하나하나 감안한 지나치게 복잡한 과적합 모델도 아닌, 학습 데이터의 패턴을 잘 반영하면서도 복잡하지 않은 **균형 잡힌** 모델을 의미

### **5-3. 편향-분산 trade-off**
- ML 문제에서 극복해야 할 가장 중요한 이슈
- 매우 단순화된 모델은 지나치게 한 **방향성**으로 치우친 경향이 있음 -> ```고편향성(High Bias)```
- 학습 데이터 하나하나의 특성을 반영하는 매우 복잡한 모델은 지나치게 높은 **변동성**을 가지게 됨 -> ```고분산성(High Variance)```

<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/da82fdf7-c098-426f-90db-8d999d5364dc" width = 400 height = 400>

- ```저편향/저분산```: 예측 결과가 실제 결과에 매우 잘 근접하면서도 예측 변동이 크지 않고 특정 부분에 집중돼 있는 아주 뛰어난 성능을 보여줌
- ```저편향/고분산```: 예측 결과가 실제 결과에 근접하지만, 예측 결과가 실제 결과를 중심으로 꽤 넓은 부분에 분포돼 있음
- ```고편향/저분산```: 정확한 결과에서 벗어나면서도 예측이 특정 부분에 집중돼 있음
- ```고편향/고분산```: 정확한 예측 결과를 벗어나면서도 넓은 부분에 분포돼 있음

<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/794c0833-42af-4b1c-b55b-c01ee7bf8f33" width = 600 height = 400>

- 일반적으로 편향과 분산은 한쪽이 높으면 한쪽이 낮아지는 경향이 있음
  - 즉, 편향이 높으면 분산은 낮아지고(과소적합) 반대로 분산이 높으면 편향이 낮아짐(과적합)
- 편향과 분산이 서로 트레이드오프를 이루면서 오류 Cost 값이 최대로 낮아지는 모델을 구축하는 것이 가장 효율적인 머신러닝 예측 모델을 만드는 방법임
  













