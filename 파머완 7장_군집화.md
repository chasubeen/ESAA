# **1. K-평균 알고리즘 이해**
- 군집화(clustering)에서 가장 일반적으로 사용되는 알고리즘
- 군집 **중심점(centroid)** 이라는 특정한 임의의 지점을 선택해 해당 중심에 가장 가까운 포인트들을 선택하는 군집화 기법
  - 선택된 포인트의 평균 지점으로 이동하고 이동된 중심점에서 다시 가까운 포인트를 선택, 다시 중심점을 평균 지점으로 이동하는 프로세스를 반복적으로 수행
  - 모든 데이터 포인트에서 더이상 중심점의 이동이 없을 경우에 반복을 멈추고 해당 중심점에 속하는 데이터 포인트들을 군집화하는 기법
<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/c02f5d18-f717-49db-ad9a-97e43ed9caef" width = 500 height = 150>

### **1-1. 군집화 프로세스**

<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/7620a2d5-ae40-42b1-84b7-85745fc29063" width = 600 height = 400>

### **1-2. K-평균의 장/단점**
**✔ 장점**  
- 일반적인 군집화에서 가장 많이 활용되는 알고리즘임
- 알고리즘이 쉽고 간결함
- 거리 기반 알고리즘
  - 개별 군집 내의 데이터가 원형으로 흩어져 있는 경우에 매우 효과적으로 군집화가 수행될 수 있음
 
**✔ 단점**  
- 거리 기반 알고리즘으로 속성의 개수가 매우 많을 경우 군집화 정확도가 떨어짐
  - 이를 위해 PCA로 차원 축소를 적용해야 할 수도 있음
- 반복을 수행할 때 반복 횟수가 많을 경우 수행 시간이 매우 느려짐
- 몇 개의 군집(cluster)을 선택해야 할 지 가이드하기 어려움

### **1-3. 사이킷런 KMeans 클래스 소개**
- KMeans 클래스

```Python
class sklearn.cluster.KMeans(n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=1, algorithm='auto')
```

**✔ 중요 파라미터**  
- ```n_clusters```: 군집화할 개수, 즉 군집 중심점의 개수를 의미함
- ```init```: 초기에 군집 중심점의 좌표를 설정할 방식, 보통은 임의로 중심을 설정하지 않고 일반적으로 k-means++ 방식으로 최초 설정
- ```max_iter```: 최대 반복 횟수, 이 횟수 이전에 모든 데이터의 중심점 이동이 없으면 종료함

**✔ 주요 속성**  
- ```labels_```: 각 데이터 포인트가 속한 군집 중심점 레이블
- ```cluster_centers_```: 각 군집 중심점 좌표(shape: [군집 개수, 피처 개수]), 이를 통해 군집 중심점 좌표가 어디인지 시각화 가능

### **1-5. 군집화 알고리즘 테스트를 위한 데이터 생성**
- 사이킷런은 다양한 유형의 군집화 알고리즘을 테스트하기 위한 간단한 데이터 생성기를 제공함
- 대표적인 군집화용 데이터 생성기: ```make_blobs()```, ```make_classification()``` API
  - 여러 개의 클래스에 해당하는 데이터 세트 생성
  - 하나의 클래스에 여러 개의 군집이 분포될 수 있게 데이터를 생성할 수 있음
  - ```make_blobs()```는 개별 군집의 중심점과 표준 편차 제어 기능이 추가돼 있음
  - ```make_classification()```은 노이즈를 포함한 데이터를 만드는 데 유용하게 사용할 수 있음
- 이외에 ```make_circle()```, ```make_moon()``` API는 중심 기반의 군집화로 해결하기 어려운 데이터 세트를 만드는 데 사용함

**✔ make_blobs() 시용법**  
- 호출 parameters
  - ```n_samples```: 생성할 총 데이터 개수, default = 100
  - ```n_features```: 데이터의 피처 개수, 시각화를 목표로 할 경우 2개로 설정해 보통 첫 번째 피처는 x 좌표, 두 번째 피처는 y 좌표상에 표현함
  - ```centers```: int값으로 설정 시 군집의 개수를 나타냄, 그렇지 않고 ndarray 형태로 표현 시 개별 군집 중심점의 좌표를 의미
  - ```cluster_std```: 생성될 군집 데이터의 표준 편차를 의미, 군집별로 서로 다른 표준 편차를 가진 데이터 세트를 만들 때 활용
    - 작을수록 군집 중심에 데이터가 모여 있으며, 클수록 데이터가 퍼져 있음을 알 수 있음
  <img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/3cf744a6-7263-4f61-9789-92d37a5ebcaf" width = 500 height = 150>

- make_blobs() 호출 시 피처 데이터 세트와 타깃 데이터 세트가 튜플(tuple)로 반환됨


# **2. 군집 평가(Cluster Evaluation)**
- 대부분의 군집화 데이터 세트는 비교할 만한 타깃 레이블을 가지고 있지 않음
- 군집화는 분류와 유사해 보일 수 있으나 성격이 많이 다름
  - 데이터 내에 숨어 있는 별도의 그룹을 찾아서 의미를 부여하거나 동일한 분류 값에 속하더라도 그 안에서 더 세분화된 군집화를 추구하거나 서로 다른 분류 값의 데이터도 더 넓은 군집화 레벨화 등의 영역을 가지고 있음
- 비지도학습의 특성상 어떠한 지표라도 정확하게 성능을 평가하기는 어려움
  - 하지만, 대략적인 성능 비교는 가능함

### **2-1. 실루엣 분석(Silhouette Analysis)**
- 각 군집 간의 거리가 얼마나 효율적으로 분리돼 있는지를 나타냄
  - 효율적으로 잘 분리됐다는 것은 다른 군집과의 거리는 떨어져 있고 동일 군집끼리의 데이터는 서로 가깝게 잘 뭉쳐 있다는 의미
  - 군집화가 잘 될수록 개별 군집은 비슷한 정도의 여유공간을 가지고 떨어져 있을 것임
- **실루엣 계수**(silhouette coefficient) 를 기반으로 함
  - 개별 데이터가 가지는 군집화 지표
  - 개별 데이터가 가지는 실루엣 계수는 해당 데이터가 같은 군집 내의 데이터와 얼마나 가깝게 군집화돼 있고, 다른 군집에 있는 데이터와는 얼마나 멀리 분리돼 있는지를 나타내는 지표
  <img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/6c5bf569-71df-44a0-915c-0c628157db30" width = 600 height = 400>
  
  - i번째 데이터 포인트의 실루엣 계수 값( s(i) )
  <img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/2beed597-96f9-4d57-82c9-32513d0e891f" width = 300 height = 80>
  
  - -1에서 1 사이의 값을 가지며, 1로 가까워질수록 근처의 군집과 더 멀리 떨어져 있다는 것을 의미
  - (-) 값은 아예 다른 군집에 데이터 포인트가 할당되었음을 뜻함

**✔ 실루엣 분석을 위한 메서드**  
- ```sklearn.metrics.silhouette_samples(X,labels,metric='euclidean',**kwds)```: 인자로 X feature 데이터 세트와 각 피처 데이터 세트가 속한 군집 레이블 값인 labels 데이터를 입력해주면 각 데이터 포인트의 실루엣 계수를 계산해 반환함
- ```sklearn.metrics.silhouette_score(X,labels,metric='euclidean',sample_size=None,**kwds)```: 인자로 X feature 데이터 세트와 각 피처 데이터 세트가 속한 군집 레이블 값인 labels 데이터를 입력해주면 전체 데이터의 실루엣 계수 값을 **평균**해 반환함
  - 일반적으로 해당 값이 높을수록 군집화가 어느 정도 잘 됐다고 판단할 수 있음
  - 하지만, 무조건 이 값이 높다고 해서 군집화가 잘 됐다고 판단할 수는 없음

**✔ 좋은 군집화의 기준**  
1. 전체 실루엣 계수의 평균값, 즉 사이킷런의 ```silhouette_score()``` 값은 0~1 사이의 값을 가지며, 1에 가까울수록 좋음  
2. 전체 실루엣 계수의 평균값과 더불어 개별 군집의 평균값의 편차가 크지 않아야 함  
즉, 개별 군집의 실루엣 계수 평균값이 전체 실루엣 계수의 평균값에서 크게 벗어나지 않는 것이 중요

### **2-3. 군집별 평균 실루엣 계수의 시각화를 통한 군집 개수 최적화**
- 전체 데이터의 평균 실루엣 계수 값이 높다고 해서 반드시 최적의 군집 개수로 군집화가 잘 됐다고 볼 수는 없음
  - 특정 군집 내의 실루엣 계수 값만 너무 높고, 다른 군집은 내부 데이터끼리의 거리가 너무 떨어져 있어 실루엣 계수 값이 낮아져도 평균적으로 높은 값을 가질 수 있음
- 군집화 도표를 활용해 군집 개수를 변화시키면서 K-평균 군집화를 수행 시 개별 군집별 평균 실루엣 계수 값을 시각화해서 군집의 개수를 정하는 데 도움을 얻을 수 있음
- 실루엣 계수를 통한 K-Means 군집 평가 방법은 직관적으로 이해하기는 쉽지만, 각 데이터별로 다른 데이터와의 거리를 반복적으로 계산해야 하므로 데이터 양이 늘어나면 수행 시간이 크게 늘어난다는 단점이 존재
  - 해당 경우 군집별로 임의의 데이터를 샘플링해 실루엣 계수를 평가하는 방안을 고민해야 함


# **3. 평균 이동(Mean Shift)**
### **3-1. 평균 이동의 개요**
- K-평균과 유사하게 중심을 군집의 중심으로 지속적으로 움직이면서 군집화를 수행
  - K-평균은 중심에 소속된 데이터의 평균 거리 중심으로 이동
  - 평균 이동은 중심을 데이터가 모여 있는 밀도가 가장 높은 곳으로 이동
- 데이터의 **분포도**를 이용해 군집 중심점을 탐색
  - **확률 밀도 함수**(probability density function)
  - 가장 집중적으로 데이터가 모여있어 확률 밀도 함수가 피크인 점을 군집 중심점으로 선정하며 일반적으로 주어진 모델의 확률 밀도 함수를 찾기 위해 **KDE**(Kernel Density Estimation)를 활용

- 평균 이동 군집화는 특정 데이터를 반경 내의 데이터 분포 확률 밀도가 가장 **높은** 곳으로 이동하기 위해 주변 데이터와의 거리 값을 **KDE 함수** 값으로 입력한 뒤 그 반환 값을 현재 위치에서 업데이트하면서 이동하는 방식을 취함

<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/e0f3efef-eedf-4265-b7aa-0cf6e15d0367" width = 600 height = 300>

**✔ KDE**(Kernel Density Estimation)  
- 함수를 통해 어떤 변수의 확률 밀도 함수를 추정하는 대표적인 방법
  - 관측된 데이터 각각에 커널 함수를 적용한 값을 모두 더한 뒤 데이터 건수로 나눠 확률 밀도 함수를 추정
- 개별 관측 데이터에 커널 함수를 적용한 뒤, 해당 적용 값을 모두 더한 후 개별 관측 데이터의 건수로 나눠 확률 밀도 함수를 추정
  - 대표적인 커널 함수: 가우시안 분포 함수
  <img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/f4dd1a92-31d3-4d89-abb8-95651d5f7056" width = 500 height = 200>  


  - KDE 커널 함수식
  <img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/dd5ba940-c357-4205-b1a5-0796563d9624" width = 400 height = 80>

    - h:대역폭, KDE 형태를 부드러운 형태로 평활화하는 데 적용/ h에 따라 확률 밀도 추정 성능이 크게 좌우됨
    - h 값에 따른 KDE 변화
      - 작은 h값: 좁고 뾰족한 KDE, 변동성이 큰 방식으로 확률 밀도 함수를 추정 -> 과적합의 위험성
      - 큰 h값: 과도하게 평활화된 KDE, 지나치게 단순화된 방식으로 확률 밀도 함수를 추정 -> 과소적합의 위험성
 
      <img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/b07f1726-2be0-455e-a35a-e8dfe6f854ed" width = 500 height = 400>
    
- 일반적으로 평균 이동 군집화는 대역폭이 **클수록** 평활화된 KDE로 인해 **적은** 수의 군집 중심점을 가지며, 대역폭이 **적을수록** **많은** 수의 군집 중심점을 가짐
- 평균 이동 군집화는 군집의 개수를 지정하지 x
  - 오직 **대역폭**의 크기에 따라 군집화를 수행
- 사이킷런의 ```MeanShift``` 클래스를 활용
  - 가장 중요한 초기화 파라미터: ```bandwidth```
  - KDE의 대역폭 h와 동일
  - 사이킷런은 최적의 대역폭 계산을 위해 ```estimate_bandwidth()``` 함수를 제공함
    - ```estimate_bandwidth()```의 파라미터로 피처 데이터 세트를 입력해주면 최적화된 bandwidth 값을 반환해줌 

- 평균 이동도 K-평균과 유사하게 중심을 가지고 있음
  - ```cluster_centers_``` 속성으로 군집 중심 좌표를 표시할 수 있음 

**✔ 평균 이동의 장/단점**  
- 장점
  - 데이터 세트의 형태를 특정 형태로 가정한다든가, 특정 분포도 기반의 모델로 가정하지 않기에 좀 더 유연한 군집화가 가능함
  - 이상치의 영향도 크지 않음
  - 미리 군집의 개수를 정할 필요가 x

- 단점
  - 알고리즘의 수행 시간이 오래 걸림
  - bandwidth의 크기에 따른 군집화 영향도가 매우 큼


# **4. GMM(Gaussian Mixture Model)**
### **4-1. GMM 소개**
- 군집화를 적용하고자 하는 데이터가 여러 개의 **가우시안 분포**를 가진 데이터 집합들이 섞여서 생성된 것이라는 가정 하에 군집화를 수행하는 방식
  - 전체 데이터 세트는 서로 다른 정규 분포 형태를 가진 여러 가지 확률 분포 곡선으로 구성될 수 있으며, 이러한 서로 다른 정규 분포에 기반해 군집화를 수행하는 방식
  - 데이터 세트를 구성하는 여러 개의 정규 분포 곡선을 추출하고, 개별 데이터가 이 중 어떤 정규 분포에 속하는지 결정하는 방식  
=> 이를 **모수 추정** 이라고 함

<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/e6918344-ac95-4cd6-88c2-66ceaa2cf3e8" width = 500 height = 250>

- 모수 추정은 주로 **2가지**를 추정하는 작업임
  1) 개별 정규 분포의 평균과 분산
  2) 각 데이터가 어떤 정규 분포에 해당되는지의 확률
- 모수 추정을 위해 GMM은 EM(Expectation and Maximization) 방법을 적용
- 사이킷런에서는 GMM의 EM 방식을 통한 모수 추정 군집화를 지원하기 위해 ```GaussianMixture``` 클래스를 지원함
  - 중요 파라미터: ```n_components```
  - gaussian mixture의 모델의 총 개수를 의미 -> 군집의 개수를 정하는 데 중요한 역할을 수행

### **4-3. GMM vs K-Means**
- K-Means는 **원형**의 범위에서 군집화를 수행
  - 데이터가 원형의 범위를 가질수록 군집화 효율이 더욱 높아짐

<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/2febe7f9-49e2-4b13-ac9f-4f6ac802dbb8" width = 450 height = 300>

- KMeans는 대표적으로 데이터가 **길쭉한** 타원형으로 늘어선 경우 군집화를 잘 수행하지 못함
- GMM이 KMeans보다 유연하게 다양한 데이터 세트에 잘 적용될 수 있다는 장점이 있음
  - 하지만, 군집화를 위한 수행 시간이 오래 걸린다는 단점이 존재

# **5. DBSCAN(Density Based Spatial Clustering of Applications with Noise)**
### **5-1. DBSCAN 개요**
- 밀도 기반 군집화 방식
- 특정 공간 내에 데이터 **밀도** 차이를 기반으로 군집화를 수행
  - 데이터의 분포가 기하학적으로 **복잡한** 데이터 세트에도 효과적인 군집화가 가능

<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/0bc82e9f-0487-4109-bb34-8cb74e649089" width = 400 height = 250>

- 주요 파라미터
  - ```epsilon```: 입실론 주변 영역, 개별 데이터를 중심으로 입실론 반경을 가지는 원형의 영역
  - ```min_points```: 개별 데이터의 입실론 주변 영역에 포함되는 타 데이터의 개수
- 입실론 주변 영역 내에 포함되는 최소 데이터 개수를 충족시키는가 아닌가에 따라 데이터 포인트를 다음과 같이 정의함
  - ```핵심 포인트```(Core Point): 주변 영역 내에 최소 데이터 개수 이상의 타 데이터를 가지고 있을 경우 해당 데이터를 핵심 포인트라고 함
  - ```이웃 포인트```(Neighbor Point): 주변 영역 내에 위치한 타 데이터를 이웃 포인트라고 함
  - ```경계 포인트```(Border Point): 주변 영역 내에 최소 데이터 개수 이상의 이웃 포인트를 가지고 있지 않지만 핵심 포인트를 이웃 포인트로 가지고 있는 데이터를 경계 포인트라고 함
  - ```잡음 포인트```(Noise Point): 최소 데이터 개수 이상의 이웃 포인트를 가지고 있지 않으며, 핵심 포인트도 이웃 포인트로 가지고 있지 않는 데이터를 잡음 포인트라고 함
ex) 파머완 p.442 ~ p.445

- DBSCAN은 입실론 주변 영역의 최소 데이터 개수를 포함하는 밀도 기준을 충족시키는 데이터인 **핵심 포인트**를 연결하면서 군집화를 수행하는 방식
- 사이킷런은 ```DBSCAN``` 클래스를 통해 해당 알고리즘을 지원
- 주요 초기화 파라미터
  - ```eps```: 입실론 주변 영역의 반경
  - ```min_samples```
    - 핵심 포인트가 되기 위해 입실론 주변 영역 내에 포함돼야 할 데이터의 최소 개수
    - 자기 자신의 데이터도 **포함**(min points + 1)  
- DBSCAN은 군집의 개수를 알고리즘에 따라 **자동**으로 지정
  - 적절한 eps와 min_samples 파라미터를 통해 최적의 군집을 찾는 과정이 중요
  - 일반적으로 eps의 값을 크게 하면 반경이 커져 포함하는 데이터가 많이지므오 노이즈 데이터의 개수가 작아짐
  - min_samples를 크게 하먄 주어진 반경 내에서 더 많은 데이터를 포함시켜야 하므로 노이즈 데이터 개수가 커지게 됨
    - 데이터 밀도가 더 커져야 하는데, 매우 촘촘한 데이터 분포가 아닌 경우 노이즈로 인식하기 때문
- 군집화 시 **-1**의 의미
  - 노이즈에 속하는 군집
  

# **6. 군집화 실습_고객 세그먼테이션**
### **6-1. 고객 세그먼테이션의 정의와 기법**
- 고객 세그먼테이션(Customer Segmentation)
  - 다양한 기준으로 고객을 분류하는 기법을 지칭
  - CRM이나 마케팅의 중요 기반 요소임
  - 주로 기업에서 어떤 상품을 얼마나 많은 비용을 써서 얼마나 자주 사용하는가에 기반한 정보로 고객들을 분류
  - 주요 목표: 타깃 마케팅
    - 고객을 여러 특성에 맞게 세분화해서 그 유형에 따라 맞춤형 마케팅이나 서비스를 제공하는 것
  - 고객의 어떤 요소를 기반으로 군집화할 것인가를 결정하는 것이 중요함
    - 기본적으로 RFM 기법을 활용
      - Recency(R): 가장 최근 상품 구입일에서 오늘까지의 기간
      - Frequency(F): 상품 구매 횟수
      - Monetary Value(M): 총 구매 금액
    

































