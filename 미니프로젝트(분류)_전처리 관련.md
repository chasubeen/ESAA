# **0. 데이터 전처리란?**
- 결측치 처리, 이상치 제거, 데이터 단위 변환, 데이터 분포 변환 등 
  - 데이터를 정확하게 분석하기 위해 먼저 데이터에 여러 가지 처리를 해주는 것
- 전처리를 함으로써 데이터 분석이 가능하도록 하며, 데이터를 합치거나 나눠서 더 정확한 정보를 갖도록 해줌
  - 전처리 과정은 데이터 분석에 있어서 반드시 필요한 부분
- 데이터를 계산하는 컴퓨터는 오로지 숫자(정수, 실수)만을 인식
  - 한국어나 영어와 같은 문자나 비어있는 값(결측치) 등을 숫자로 변경해 주어야 함 => **Encoding** 
   
### **0-1. 양적 변수 vs 질적 변수**
- ```양적(quantitative)``` 변수
  - 변수의 값을 숫자로 나타낼 수 있는 변수 ex> 키, 몸무게, 소득 등
  - 수치형 변수
  - 이산변수와 연속변수로 구분
    - 이산(discrete) 변수: 하나하나 셀 수 있는 변수 ex> 정수
    - 연속(continuous) 변수: 이산변수와 다르게 변수의 값 사이에 무수히 많은 또 다른 값들이 존재하는 경우 ex> 실수
- ```질적(qualitative)``` 변수
  - 변수의 값이 비수치적 특정 카테고리에 포함시키도록 하는 변수 ex> 색상, 성별, 종교
  - 범주형 변수
    - 명목(nomial) 변수: 변수의 값이 특정한 범주에 들어가지만 해당 범주간 순위는 존재하지 않는 경우
    - 순위(ordinal) 변수: 변수의 값이 특정 범주에 들어가면서 변수의 값이 순위를 가지는 경우 

- 현재 프로젝트에서 ```quality```, ```type```을 제외한 변수들은 모두 **양적** 변수

### **0-2. EDA(Exploratory Data Analysis)**
- ```df.info()```를 통해 데이터의 정보 확인
- ```df.describe()```를 통해 양적 변수들의 기술통계량(빈도, 평균, 최대/최소, 사분위값) 파악
- ```df.hist()```를 통해 양적 변수들의 분포를 히스토그램으로 파악
- ```sns.countplot(x = df[columns])```를 통해 질적 변수들의 빈도 시각화
- ```sns.boxplot(y = df[columns])```를 통해 이상치 탐지(상자 수염 그림)
- ```sns.heatmap(df.corr())```를 통해 변수들 간의 상관계수 시각화
- ```scipy.stats.skew()```를 통해 데이터의 왜곡 정도 파악
  - 카테고리형 변수(범주형 변수)의 경우 원-핫 인코딩 시 데이터가 왜곡될 가능성이 높음 => ```skew()``` 함수 적용 시에는 원-핫 인코딩이 적용되지 않은 데이터로  

### **0-3. 범주형 변수 전처리**
#### **a) 라벨 인코딩(Label Encoding)**
- 카테고리 피처를 코드형 숫자 값으로 변환하는 것
- sklearn의 ```LabelEncoder``` 클래스로 구현
```Python
from sklearn.preprocessing import LabelEncoder

items = ['사과', '배', '바나나']

encoder = LabelEncoder() # 객체 생성
encoder.fit(items) # 학습
labels = encoder.transform(items) # 라벨 변환(인코딩)
```

- ```classes_``` 속성을 통해 문자열 값이 어떤 숫자 값으로 인코딩됐는지 확인할 수 있음
- ```inverse_transform()```을 통해 인코딩된 값을 다시 디코딩할 수 있음

- 숫자 값의 크고 작음에 대한 특성이 작용할 위험성이 존재  
  -> 분류는 괜찮지만 선형 회귀 등의 ML 알고리즘에는 적용을 지양해야 함
- 숫자의 이러한 특성 반영 해결을 위해 **원-핫 인코딩** 활용
  - 그러나 원핫 인코딩의 경우 변수의 개수가 많아진다는 단점 존재

> 현재는 **분류** 문제이므로 **라벨 인코딩**을 활용

# **1. 변수들 간의 상관도**
### **1-1. target 변수(quality)와의 상관도**
- target 변수와 높은 상관관계(0.7 이상)를 가지는 변수는 관찰되지 x
- 가장 상관도가 높은 변수는 ```alcohol```
  - 이마저도 **0.44**로 상관도가 높다고 판단하기엔 힘듦
  - 해당 변수에서 이상치가 있다면 제거하는 것도 고려해 볼 수 있음   
    > 파머완 p. 273 ~ 274
 
### **1-2. 각 feature 변수들 간의 상관도**
- 회귀 문제의 경우 feature 변수들 간의 상관도가 높은 경우 **다중공선성(multicollinearity)** 문제가 발생할 수 있음
  - 모델 성능의 저하
- 분류 문제의 경우 feature 변수들 간의 상관도가 높아도 모델의 성능에는 큰 영향을 끼치지는 않는다..?!


# **2. Class 불균형**
- 해당 분류 문제는 와인의 **품질(quality)** 을 0에서부터 10까지의 클래스로 분류하는 문제
  - 다중 분류 문제(Multi-class Classification) 
  - 근데 train 데이터에는 3부터 9까지밖에 없는데 이걸로만 구분해도 되는걸까?
- train 데이터에서 클래스 간 분포가 불균형함(imbalanced)
- 두 가지 접근 방식으로 해결할 수 있음
  - Cost function based approach: rare class에 가중치를 더 주거나, rare class를 잘못 분류했을 때 cost/loss/param을 조정하는 방법
  - Sampling based approach: 데이터 자체를 늘리고 줄이는 방법  
    ex) 업샘플링, 다운샘플링, SMOTE

> 파머완 p.275 ~ 278  
> [imbalance classification 다루는 방법](https://chealin93.tistory.com/118)  
> [Class imbalance(클래스 불균형)이란?](https://techblog-history-younghunjo1.tistory.com/74)  

### **2-1. Oversampling & Undersampling**

#### **a) Oversampling**  
<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/b6e1653d-087e-4994-b816-a90c3da04bad" width = 300 height = 150>

- 이상 데이터와 같이 적은 데이터 세트를 **증식**하여 학습을 위한 충분한 데이터를 확보하는 방법
  - 원본 데이터의 피처 값들을 아주 약간 변경하여 증식
  - 대표적으로 SMOTE 방법이 O  
- ```SMOTE```  

  <img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/02a1efa3-7fad-41fa-bef0-d8a81e98d55f" width = 450 height = 150>
  
  - Synthetic Minority Over-sampling TEchnique
  - 적은 데이터 세트에 있는 개별 데이터들의 K-최근접 이웃(K-NearestNeighbor)을 찾아 이 데이터와 K개 이웃들의 차이를 일정 값으로 만들어 기존 데이터와 약간 차이가 나는 새로운 데이터 생성하는 방식
  - 단계>  
    1. 소수 클래스의 데이터 중 특정 벡터(샘플)와 가장 가까운 k개의 이웃 벡터 선정
    2. 기준 벡터와 선정한 벡터 사이를 선분으로 연결
    3. 선분 위의 임의의 점이 새로운 벡터(or 이 중 임의로 하나 선택)
    <img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/b2ac679f-a6f4-4feb-a9a5-fd65749562c2" width = 300 height = 200>
 
#### **b) Undersampling**
<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/728b7b2d-7d16-4fe3-a807-0c182828fd24" width = 300 height = 150>

- 많은 데이터 세트를 적은 데이터 세트 수준으로 **감소** 시키는 방식
- 장점: 과도하게 정상 레이블로 학습/예측하는 부작용 개선
- 단점: 너무 많은 정상 레이블 데이터를 감소시켜 정상 레이블의 경우 오히려 제대로 된 학습 수행 불가

### **2-2. 클래스 가중치(Weight Balancing)**
- 클래스의 비율에 대해 가중치를 두는 방법
  - ex> 두 개의 클래스 비율이 1:9라면 가중치를 9:1로 줌으로써 전체 클래스의 loss에 동일하게 기여하도록
- 주로 딥러닝 모델에서 손실함수(loss function)을 통해 가중치 부여
  - sklearn에 관련 API가 존재하기는 함

#### **sklearn의 compute_class_weight**
- 불균형한 데이터에 대해 각 클래스의 가중치를 계산해 줌
  - i번째 클래스의 가중치가 class_weight_vect[i]인 배열로 반환
- 코드>  

```Python
sklearn.utils.class_weight.compute_class_weight(class_weight, *, classes, y)
```

- Parameters>
  - ```class_weight```
    - dict 형태로 입력
    - 'balanced'를 선택 시 클래스 가중치는 (샘플의 수)/(클래스 수 * y의 각 클래스 별 빈도수)로 부여됨
    - 딕셔너리를 파라미터로 넘기면 클래스 별로 해당하는 클래스 가중치를 계산
    - 'None'인 경우 균일 분포로 클래스 가중치가 계산됨
    
  - ```classes```
    - ndarray
    - 데이터 내의 클래스 목록 
  - ```y``` 
    - 배열 형태
    - 각 샘플에 대한 원본 클래스 라벨값

# **3. 이상치/ 데이터 분포에 대한 전처리**
### **3-1. 피처 스케일링(Feature Scaling)**
- 서로 다른 변수의 값 범위를 일정한 수준으로 맞추는 작업
- 종류
  - ```표준화(StandardScaler)```: 개별 피처를 평균이 0이고, 분산이 1인 값으로 변환
  - ```정규화(MinMaxScaler)```: 데이터 값을 0과 1 사이의 범위 값으로 변환
  - ```표준 정규화(RobustScaler)```: 데이터에 이상치에 많을 때 활용  
    cf> 구글링으로 비슷한 대회 하신 분 포스팅을 찾다가 발견한 거였는데 RobustScaler 적용 시에 ~성능이 더 떨어졌다고..^^;~    
※ 학습 데이터로 fit()이 적용된 스케일링 기준 정보를 그대로 테스트 데이터에 적용해야 함    

  - 가능하다면 전체 데이터의 스케일링 변환 후 train, test 데이터 분리
  - 또는 train data에 대해서는 ```fit_transform()```, test data에 대해서는 ```transform()```  

### **3-2. 데이터 변환(Data Transformation)**
- 데이터가 **왜곡된** 분포를 가지는 경우 데이터 변환을 통해 왜곡 정도를 완화할 수 있음
- ```pH``` 변수는 정규 분포를 따르는 것처럼 보인다.
- 그 외의 feature들은 모두 분포에 편향이 있다고 판단된다.(오른쪽으로 긴 꼬리, **positively skewed**)
- 데이터 변환 방법: 제곱근(square root), 세제곱근(cube root), 로그 변환(log transformation)


 





































































