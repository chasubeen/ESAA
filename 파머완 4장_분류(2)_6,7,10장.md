# **1. XGBoost**

### **1-1. XGBoost 개요**
- 트리 기반의 앙상블 학습에서 가장 각광받고 있는 알고리즘 중 하나
- 분류에 있어서 다른 머신러닝보다 뛰어난 예측 성능을 나타냄
- GBM 기반이지만, GBM의단점인 느린 수행 시간 및 과적합 규제 부재 등의 문제를 해결한 알고리즘
- 병렬 CPU환경에서의 병렬 학습을 지원하여 기존 GBM보다 빠른 학습 가능

(227 표 추가하기)

### **1-2. XGBoost 설치**
- 파머완 p.228 ~ p.229

### **1-3. 파이썬 래퍼 XGBoost 하이퍼 파라미터**
(p.231 사진 추가) 
#### **✅ 파이썬 래퍼 XGBoost 파라미터**
- ```일반``` 파라미터
	- 일반적으로 실행 시 스레드의 개수나 silent 모드 등의 선택을 위한 파라미터
  - 디폴트 파라미터 값을 바꾸는 경우는 거의 x
- ```부스터``` 파라미터: 트리 최적화, 부스팅, 규제(regularization) 등과 관련된 파라미터 등을 지칭
- ```학습 태스크``` 파라미터
	- 학습 수행 시의 객체 함수 
	- 평가를 위한 지표 등을 설정하는 파라미터












# **2. LightGBM**
- XGBoost와 함께 **부스팅** 계열 알고리즘에서 각광받고 있는 모델
- 장점
  - XGBoost보다 학습에 걸리는 시간이 훨씬 **적음**
  - 메모리 사용량이 적음
  - 카테고리형 피처의 자동 변환과 최적 분할(One-hot Encoding 등을 사용하지 않고도 카테고리형 피처를 최적으로 변환하고 이에따른 노드 분할 수행)
  - 대용량 데이터에 대한 뛰어난 예측 성능 및 병렬 컴퓨팅 기능을 제공하고 있음
    - 추가적으로 최근에는 GPU도 지원하고 있음 
- 단점
  - 적은 데이터 세트(일반적으로 10000건 이하의 데이터 세트)에 적용할 경우 과적합이 발생하기 쉬움
- 일반 GBM 계열의 트리 분할 방법과 다르게 **리프 중심 트리 분할(Leaf Wise)** 방식을 활용
  - 대부분은 트리의 깊이를 효과적으로 줄이기 위한 **균형 트리 분할(Level Wise)** 방식을 활용
  - 리프 중심 트리 분할 방식의 경우 트리의 균형을 맞추지 않고, 최대 손실 값(max delta loss)을 가지는 리프 노드를 지속적으로 분할하면서 트리의 깊이가 깊어지고 비대칭적인 규칙 트리를 생성함 
  - 최대 손실값을 가지는 리프 노드를 지속적으로 분할해 생성된 규칙 트리는 학습을 반복할수록 결국은 균형 트리 분할 방식보다 예측 오류 손실을 최소화 할 수 있다는 것이 LightGBM의 구현 사상

<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/a2ca08fb-6ce9-40be-836c-640c29690a49" width = 500 height = 150>


### **2-1. LightGBM 설치**
- 파머완 p.246 ~ 247

### **2-2. LightGBM 하이퍼 파라미터**
- LightGBM은 XGBoost와 다르게 리프 노드가 계속 **분할**되면서 트리의 깊이가 깊어지므로 이러한 트리 특성에 맞는 하이퍼 파라미터 설정이 필요하다는 점
  - 예) ```max_depth```를 매우 크게 가짐

#### **주요 파라미터**  
- ```num_iterations(default = 100)```
  - 반복 수행하려는 트리의 개수를 지정
  - 크게 지정할수록 예측 성능이 높아질 수 있으나, 너무 크게 지정하면 오히려 과적합으로 성능이 저하될 수 있음
  - 이후 사이킷런 호환 클래스에서는 ```n_estimators```로 이름이 변경됨 
- ```learning_rate(default = 0.1)```
  - 0에서 1 사이의 값을 지정하며 부스팅 스텝을 반복적으로 수행할 때 업데이트되는 학습률 값
  - 일반적으로 ```n_estimators```를 **크게** 하고 ```learning_rate```를 **작게** 해서 예측 성능을 향상시킬 수 있으나, 마찬가지로 과적합 이슈와 학습 시간이 길어지는 부정적인 영향 또한 고려해야 함
- ```max_depth(default = -1)```
  - 트리 기반 알고리즘의 ```max_depth```와 동일
  - 0보다 작은 값을 지정하면 깊이에 제한이 없음
  - Depth Wise 방식의 트리와 다르게 LightGBM은 Leaf Wise 기반이므로 깊이가 상대적으로 더 깊음
- ```min_data_in_leaf(default = 20)```
  - 결정 트리의 ```min_samples_leaf```와 같은 파라미터
  - 최종 결정 클래스인 리프 노드가 되기 위해서 최소한으로 필요한 레코드 수
  - 과적합을 제어하기 위한 파라미터
- ```num_leaves(default = 31)```
  - 하나의 트리가 가질 수 있는 최대 리프 개수 
- ```boosting(default = gbdt)```
  - 부스팅의 트리를 생성하는 알고리즘을 기술 
  - gbdt: 일반적인 그래디언트 부스팅 결정 트리
  - rf: 랜덤 포레스트
- ```bagging_fraction(default = 1.0)```
  - 트리가 커져서 과적합되는 것을 제어하기 위해서 데이터를 샘플링하는 비율을 지정
- ```feature_fraction(default = 1.0)```
  - 개별 트리를 학습할 때마다 무작위로 선택하는 feature의 비율
  - 과적합을 막기 위해 사용됨 
- ```lambda_l2(default = 0.0)```
  - L2 규제를 위한 값 
  - 피처 개수가 많을 경우 적용을 검토하며 값이 클수록 과적합 감소 효과가 있음
- ```lambda_l1(default = 0.0)```
  - L1 규제 제어를 위한 값
  - L2와 마찬가지로 과적합 제어를 위한 것

### **2-3. Learning Task 파라미터**
- ```objective```
  - 최솟값을 가져야 할 손실함수를 정의
  - XGBoost의 objective 파라미터와 동일
  - 애플리케이션 종류(회귀, 다중 클래스 분류, 이진 분류)에 따라 objective인 손실함수가 지정됨 

### **2-4. 하이퍼 파라미터 튜닝 방안**
- ```num_leaves``` 개수를 중심으로 ```min_child_samples(min_data_in_leaf)```, ```max_depth```를 함께 조정하면서 모델의 복잡도를 줄이는 방향이 기본 튜닝 방안임
  - 일반적으로 ```num_leaves```의 개수를 높이면 정확도가 높아지지만, 반대로 트리의 깊이가 깊어지고 모델이 복잡도가 커져 과적합 영향도가 커짐
  - ```min_child_samples```는  ```num_leaves```와 학습 데이터의 크기에 따라 달라지지만, 보통 큰 값으로 설정하면 트리가 깊어지는 것을 방지
  - ```max_depth```는 명시적으로 깊이의 크기를 제한함
- 모두 과적합을 개선하는 데 활용됨
- 이외에도 ```reg_lambda```, ```reg_alpha```와 같은 정규화를 적용하거나 학습 데이터에 사용할 피처의 개순나 데이터 샘플링 레코드 갯구를 줄이기 위해 ```colsample_bytree```, ```subsample``` 파라미터를 적용할 수 있음

### **2-5. 파이썬 래퍼 LightGBM vs 사이킷런 래퍼 XGBoost, LightGBM 하이퍼 파라미터 비교**

<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/ddd4bc4f-cc49-45cc-be4f-71b48d32fba6" width = 800 height = 350>


# **3. 스태킹 앙상블**

























