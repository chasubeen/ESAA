# **1. 분류(Classification)의 개요**
- 지도 학습: 라벨(label), 즉 명시적인 정답이 있는 데이터가 주어진 상태에서 학습하는 머신러닝 방식
- 분류는 ```지도 학습```의 일종
  - 주어진 데이터의 피처와 레이블 값(결정 값, 클래스 값)을 머신러닝 알고리즘으로 학습해 모델을 생성하고, 생성된 모델에 **새로운** 데이터 값이 주어졌을 때 미지의 ```레이블 값을 예측```하는 것
  - 기존 데이터가 어떤 레이블에 속하는 지 **패턴**을 알고리즘으로 인지한 뒤에 새롭게 관측된 데이터에 대한 ```레이블```을 판별하는 것
  
- 다양한 분류 알고리즘
  - ```나이브 베이즈(Naive Bayes)```: 베이즈(Bayes) 통계와 생성 모델에 기반
  - ```로지스틱 회귀(Logistic Regression)```: 독립변수와 종속변수의 **선형** 관계성에 기반
  - ```결정 트리(Decision Tree)```: 데이터 균일도에 따른 최대 분류 마진을 효과적으로 찾아줌
  - ```서포트 벡터 머신(Support Vector Machine)```: 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아줌
  - ```최소 근접(Nearest Neighbor) 알고리즘```: 근접 거리 기준
  - ```신경망(Neural Network)```: 심층 연결 기반
  - ```앙상블(Ensemble)```: 서로 다른(또는 같은) 머신러닝 알고리즘을 결합하는 방식
    - 대부분은 **같은** 알고리즘을 결합 
    - 매우 많은 여러 개의 **약한** 학습기(-> 예측 성능이 상대적으로 떨어지는 학습 알고리즘)를 결합해 확률적 보완과 오류가 발생한 부분에 대한 ```가중치를 계속 업데이트```하면서 예측 성능을 향상시키는 방식
    - 일반적으로 **배깅(Bagging)** 과 **부스팅(Boosting)** 방식으로 나뉨
    - ```랜덤 포레스트(Random Forest)```: 대표적인 배깅 알고리즘
    - ```그레디언트 부스팅(Gradient Boosting)```, ```XGBoost```, ```LightGBM```: 부스팅 알고리즘 

# **2. 결정 트리(Decision Tree)**
- 데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 **트리** 기반의 분류 규칙을 만드는 것
  - 주로 ```if/else``` 기반( -> 스무고개)
- **결정 트리의 구조**  

<img src = "https://user-images.githubusercontent.com/98953721/235280544-db6a034b-6ed7-459f-b7f1-84af5d267108.png" width = 500 height = 250>

- 데이터 세트에 피처가 있고 이러한 피처가 결합해 규칙 조건을 만들 때마다 ```규칙 노드```가 생성됨
  - 많은 규칙이 있으면 분류를 결정하는 방식이 복잡해짐 -> **과적합(Overfitting)** 위험
  - 트리의 깊이(depth)가 **깊어질수록** 결정 트리의 예측 성능이 **저하** 가능성이 높아짐
  - 가능한 한 **적은** 결정 노드로 높은 예측 정확도를 가지도록 트리 **분할(split)** 규착 찾기

- **균일한 데이터**  
  - 데이터 세트의 균일도: 데이터를 구분하는 데 필요한 정보의 양
  <img src = "https://user-images.githubusercontent.com/98953721/235281082-2c634c01-9427-4b7f-9e94-2ffaadbe54df.png" width = 600 height =200>
  
  - 결정 노드는 정보 균일도가 **높은** 데이터 세트를 먼저 선택할 수 있도록 규칙 조건을 생성
    - 정보 균일도가 데이터 세트로 쪼개질 수 있도록 조건을 찾아 ```서브 데이터 세트```를 만들고, 다시 이 서브 데이터 세트에서 균일도가 **높은** 자식 데이터 세트 쪼개는 방식을 자식 트리로 내려가면서 반복
  
  - 정보의 균일도 측정 방식
    - ```정보 이득(Information Gain)```
      - 엔트로피(주어진 데이터 집합의 혼잡도) 개념 기반
        - 서로 다른 값이 섞여있으면 **높은** 엔트로피 
      - 정보 이득 지수: 1 - (엔트로피 지수)
        - 정보 이득이 높은 속성을 기준으로 데이터 분할 규칙을 생성
        
    - ```지니 계수(Gini)```
      - 불평등 지수
      - 0이 가장 평등하고 1로 갈수록 불평등
      - 머신러닝 적용 시엔 지니 계수가 낮을수록 데이터 균일도가 **높은** 것으로 해석

- 사이킷런의 ```DecisionTreeClassifier```는 **지니 계수**를 이용해 데이터 세트를 분할
  - 데이터 세트를 분할하는 데 ```가장 좋은 조건(-> 정보 이득이 높음/ 지니 계수가 낮음)```을 찾아서 자식 트리 노드에 걸쳐 반복적으로 분할한 뒤, 데이터가 모두 특정 분류에 속하게 되면 분할을 멈추고 분류를 결정하는 방식

<img src = "https://user-images.githubusercontent.com/98953721/235281334-dbbaea27-7cc7-47fe-be1f-3f889bdff665.png" width = 700 height = 400>

### **2-1. 결정 트리 모델의 특징**
- 장점> **균일도** 기반 분류
  - 쉽고 직관적인 알고리즘, 시각화
  - 특별한 경우릎 제외하고는 각 피처의 스케일링과 정규화 등의 전처리 작업이 필요 x
- 단점> 과적합으로 인한 정확도 감소
  - 사전에 트리의 크기를 제한하는 튜닝 필요

### **2-2. 결정 트리 파라미터**
- 결정 트리 알고리즘
  - ```DecisionTreeClassifier```: 분류를 위한 클래스
  - ```DecisionTreeRegressor```: 회귀를 위한 클래스
- ```CART``` 알고리즘 기반 

- 결정 트리는 규칙 생성 로직을 미리 제어하지 않으면 완벽하게 클래스 값을 구별해내기 위해 트리 노드를 **계속**해서 만들어 나감
  - 결국 매우 복잡한 규칙 트리가 만들어져 모델이 쉽게 **과적합**되는 문제점을 가지게 됨
  - 과적합 위험이 상당히 높은 ML 알고리즘
  - 결정 트리 알고리즘을 제어하는 대부분의 하이퍼 파라미터는 복잡한 트리가 생성되는 것을 막기 위한 용도임
  
- **Parameters>**    
  - ```min_samples_split```
    - 노드를 분할하기 위한 최소한의 샘플 데이터 수 -> 과적합 제어
    - **default = 2**, 작게 설정할수록 분할되는 노드 수 증가 -> 과적합 가능성 증가
    - 과적합 제어, 1로 설정할 경우 분할되는 노드가 많아져서 과적합 가능성 증가 
    
    <img src = "https://user-images.githubusercontent.com/98953721/235286174-d382e3b8-3c90-4fd2-a248-4c3f9ccccea8.png" width = 700 height = 500>
    
  - ```min_samples_leaf```
    - 말단 노드(leaf)가 되기 위한 최소한의 샘플 데이터 수
      - 노드 생성 규칙이 변경될 수 있음
    - 과적합 제어 용도
    - 비대칭적(imbalanced) 데이터의 경우 특정 클래스의 데이터가 극도로 작아지는 문제가 발생할 수 있음
    <img src = "https://user-images.githubusercontent.com/98953721/235286277-2455abd5-074c-4444-954b-6ece34984f89.png" width = 700 height = 500>
 
  - ```max_features```
    - 최적의 분할을 위해 고려할 최대 피처 개수, default = None
    - ```int```형으로 지정하면 대상 피처의 **개수**, ```float```로 지정하면 전체 피처 중 대상 피처의 **퍼센트**
    - ```sqrt```: 전체 피처 중 sqrt(전체 피처 개수)
    - ```auto```: sqrt와 동일
    - ```log```: 전체 피처 중 log2(전체 피처 개수)
    - ```None```: 데이터 세트의 **모든** 피처를 사용
    
  - ```max_depth```
    - 트리의 최대 깊이 규정
    - default: None -> 완벽하게 클래스 결정 값이 될 때까지 깊이를 계속 키우며 분할하거나 노드가 가지는 데이터 수가 min_samples_split보다 작아질 때까지 계속 깊이를 증가시킴
    - 깊이가 깊어지면 최대 분할로 인한 과적합이 발생할 수 있음 -> 과적합 가능성, 적절한 제어 필요
    
    <img src = "https://user-images.githubusercontent.com/98953721/235285955-2efc5588-8eea-4f81-ab98-3d36d3f697ae.png" width = 700 height = 400>
    
  - ```max_leaf_nodes```
    - 말단 노드(leaf)의 최대 개수

### **2-3. 결정 트리 모델의 시각화**
- **Graphviz** 패키지 활용
  - 사이킷런의 ```export_graphviz()``` 함수 인자로 학습이 완료된 estimator, 피처 이름 리스트, 레이블 이름 리스트 입력 시 학습된 결정 트리 규칙을 실제 트리 형태로 시각화
  - Graphviz가 읽어 들여서 그래프 형태로 시각화 할 수 있는 출력 파일 생성
  - 출력 결과를 통해 각 규칙에 따른 트리의 브랜치(branch) 노드와 말단 리프(leaf) 노드가 어떻게 구성되는지 파악 가능
  
- **결과 해석**    
  - ```petal_length(cm) <= 2.45```와 같이 피처의 조건이 있는 것은 자식 노드를 만들기 위한 규칙 조건
  - 조건이 없는 경우 **리프 노드**
  - ```gini```: 다음의 value = [ ]로 주어진 데이터 분포에서의 지니 계수
  - ```samples```: 현 규칙에 해당하는 데이터 건수
  - ```value = [ ]```: 클래스 값 기반의 데이터 건수

### **2-4. 규칙 조건 생성 과정**
- 결정 트리는 **균일도**에 기반해 어떠한 속성을 규칙 조건으로 선택하느냐가 중요함
  - 중요한 **몇 개**의 피처가 명확한 규칙 트리를 만드는 데 크게 기여
  - 모델을 좀 더 간결하고 이상치(outlier)에 강한 모델을 만들 수 있음
  - DecisionTreeClassifier 객체의 ```feature_importances_``` 속성으로 확인 가능
  
- ```feature_importances_``` 속성
  - ndarray 형태로 값을 반환, 피처 순서대로 값이 할당됨
  - 값이 **높을수록** 피처의 중요도가 높음

### **2-5. 결정 트리 과적합(Overfitting)**
<img src = "https://user-images.githubusercontent.com/98953721/235287022-cf69736e-b1f7-4415-86fa-c8bc315afe9e.png" width = 400 height = 250>

- 일부 이상치(Outlier) 데이터까지 분류하기 위해 분할이 자주 일어남 
  - 결정 기준 경계가 매우 **많아진** 것을 확인할 수 있음
- 결정 트리의 **기본 파라미터** 설정은 리프 노드 안의 데이터가 모두 균일하거나 하나만 존재해야 하는 엄격한 분할 기준을 가짐 -> 결정 기준 경계가 많아짐, 복잡해짐
- 복잡한 모델의 경우 학습 데이터 세트의 특성과 약간만 다른 형태의 데이터 세트예측 시 예측 정확도가 **떨어지는** 문제 발생

---
<img src = "https://user-images.githubusercontent.com/98953721/235287103-fa5d657d-7240-400c-a113-ded901f07549.png" width = 400 height = 250>

- 이상치에 크게 반응하지 않으면서 좀 더 일반화된 분류 규칙에 따라 분류되었음을 확인할 수 있음


# **3. 앙상블 학습**
### **3-1. 앙상블 학습 개요**
- 여러 개의 분류기(classifier)를 생성하고 그 예측을 결합함으로써 보다 정확한 최종 예측을 도출하는 기법
  - 목표: 다양한 분류기의 예측 결과를 결합함으로써 단일 분류기보다 신뢰성이 높은 예측값을 얻는 것

### **3-2. 보팅(Voting)**
- **같은** 데이터 세트에 대해 훈련된 서로 **다른** 분류기를 결합하고, 투표를 통해 최종 예측 결과를 결정하는 방식

#### **✔ 하드 보팅(Hard Voting) vs 소프트 보팅(Soft Voting)**
- 하드 보팅
  - 예측한 결괏값들 중 다수의 분류기가 결정한 예측값을 최종 보팅 결괏값으로 선정하는 것 -> 다수결 원칙
- 소프트 보팅
  - 분류기들의 레이블 값 결정 확률을 모두 더하고 이를 평균해서 이들 중 확률이 가장 높은 레이블 값을 최종 보팅 결괏값으로 선정 

- 일반적으로 하드 보팅보다는 소프트 보팅이 더 많이 활용됨(by 예측 성능) 

<img src = "https://user-images.githubusercontent.com/98953721/235332348-09a595df-48bc-4d27-9f70-9e3a5ef3e85f.png" width= 700 height = 350>

#### **✔ 보팅 분류기(Voting Classifier)**
- 사이킷런은 보팅 방식의 앙상블을 구현한 ```VotingClassifier``` 클래스를 제공하고 있음
- Parameters>  
  - ```estimators```: 리스트 값으로 보팅에 사용될 여러 개의 Classifier 객체들을 튜플 형식으로 입력받음
  - ```voting```: 보팅 방식 결정(default: 'hard') 

### **3-3. 배깅(Bagging/ Bootstrap Aggregating)**
- 각각의 분류기는 모두 **같은** 유형의 알고리즘 기반이지만, 데이터 샘플링을 서로 다르게 가져가면서 학습을 수행해 보팅을 수행하는 것
- 부트스트래핑(Bootstrapping) 분할 방식: 개별 Classifier에게 데이터를 샘플링해서 추출하는 방식(중첩 허용)  
ex> 랜덤 포레스트

<img src = "https://user-images.githubusercontent.com/98953721/235331697-44a3cda8-2e57-4c37-b2ba-8037c2c7375b.png" width = 450 height = 250>

### **3-4. 부스팅(Boosting)**
  - 여러 개의 분류기가 순차적으로 학습을 수행하되, 앞에서 학습한 분류기가 예측이 틀린 데이터에 대해서는 올바르게 예측할 수 있도록 다음 분류기에는 **가중치(weight)** 를 부여하면서 학습/예측을 수행하는 것
  - 뛰어난 예측 성능
  - ex> 그레디언트 부스트, XGBoost, LightGBM 등

### **3-5. 스태킹**
- 여러 가지 다른 모델의 예측 결과값을 다시 학습 데이터로 만들어서 다른 모델(메타 모델)로 재학습시켜 결과를 예측하는 방법


# **4. 랜덤 포레스트(Random Forest)**
### **4-1. 랜덤 포레스트의 개요 및 실습**
- 앙살블 알고리즘 중 비교적 빠른 수행 속도
- 다양한 영역에서의 높은 예측 성능
- 기반 알고리즘: 결정 트리
  - 여러 개의 결정 트리 분류기가 전체 데이터에서 배깅 방식으로 각자의 데이터를 샘플링해 개별적으로 학습을 수행한 뒤 최종적으로 모든 분류기가 보팅을 통해 예측 결정

<img src = "https://user-images.githubusercontent.com/98953721/235332773-454b6170-e680-4fd7-9c90-553b17be7e32.png" width = 500 height = 350>

- 사이킷런의 ```RandomForestClassifier``` 클래스를 통해 랜덤 포레스트 기반의 분류를 지원

### **4-2. 부트스트래핑(Bootstrap) 분할 방식**
- 여러 개의 데이터 세트를 중첩되게 분리하는 것
  - subset의 데이터 건수는 전체 데이터 건수와 동일하지만, 개별 데이터가 **중첩** 되어 만들어짐 
- 데이터가 중첩된 개별 데이터 세트에 **결정 트리** 분류기를 각각 적용하는 것이 랜덤 포레스트

### **4-3. 하이퍼 파라미터 튜닝**
- 트리 기반 앙상블 알고리즘의 단점
  - 다수의 하이퍼 파라미터
    - 튜닝을 위한 시간이 많이 소모됨
  - 튜닝 후 예측 성능이 크게 향상되는 경우가 많지 않음

- Parameters>  
  - ```n_estimators```
    - 랜덤 포레스트에서 결정 트리의 개수를 지정
    - 많이 설정할수록 좋은 성능을 기대할 수 있지만 계속 증가시킨다고 성능이 무조건 향상되는 것은 아님
    - 늘릴수록 학습 수행 시간이 오래 걸림 
  - ```max_features```
    - 결정 트리에서와 기능은 동일
    - default = 'auto'(-> 'sqrt') 
  - ```max_depth```나 ```min_samples_leaf```와 같이 결정 트리에서 과적합을 개선하기 위해 사용되는 파라미터가 랜덤 포레스트에서도 똑같이 적용될 수 있음

- ```feature_importances_``` 속성을 이용해 알고리즘이 선택한 피처의 중요도를 파악할 수 있음


# **5. GBM(Gradient Boosting Machine)**

### **5-1. GBM 개요 및 실습**
- **부스팅**: 여러 개의 약한 학습기를 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치 부여를 통해 오류를 개선해 나가면서 학습하는 방식
- 대표적인 예로 AdaBoost(Adaptive Boosting)와 그레디언트 부스트가 있음
  - ```AdaBoost```: 오류 데이터에 **가중치** 를 부여하면서 부스팅을 수행하는 대표적인 알고리즘
  - ```GBM```: 에이다부스트와 유사하지만, 가중치 업데이트를 위해 **경사 하강법**을 활용
    - 경사 하강법: 오류(실제 값 - 예측값)를 **최소화**하는 방향성을 가지고 반복적으로 가중치 값을 업데이트 하는 것
    - 사이킷런의 ```GradientBoostingClassifier``` 활용

- 예측 성능은 랜덤 포레스트에 비해 조금 더 뛰어나나, 수행 시간이 오래 걸리고 하이퍼 파라미터 튜닝에 노력이 더 필요함

### **5-2. GBM 하이퍼 파라미터 튜닝**
- ```loss```: 경사 하강법에서 사용할 비용 함수(default = 'deviance')
- ```learning_rate```
  - GBM이 학습을 진행할 때마다 적용하는 학습률 -> weak learner가 순차적으로 오류 값을 보정해 나가는 데 적용하는 계수
  - 0 ~ 1 사이의 값을 지정할 수 있음(default = 0.1)
  - 너무 작은 값을 적용하면 업데이트 되는 값이 작아져서 최소 오류 값을 찾아 예측 성능이 높아질 가능성이 있지만, 순차적인 반복으로 인해 수행 시간이 오래 걸림
  - 너무 큰 값을 적용하면 최소 오류 값을 찾지 못하고 그냥 지나쳐 버려 예측 성능이 떨어질 가능성이 높아지지만, 빠른 수행 가능
  - ```n_estimators```와 상호 보완적으로 조합하여 사용
    - ```learning_rate```를 작게 하고 ```n_estimators```를 크게 하면 더 이상 성능이 좋아지지 않는 한계점까지는 예측 성능이 조금씩 향상될 수 있음

- ```n_estimators```
  - 약한 학습기의 개수
  - weak learner가 순차적으로 오류를 보정 -> 개수가 많을수록 예측 성능이 일정 수준까지는 좋아질 수 있음
  - 개수가 많을수록 수행 시간이 오래 걸린다는 단점 존재(default = 100)

- ```subsample```
  - weak learner가 학습에 사용하는 데이터의 샘플링 비율
  - default = 1: 전체 학습 데이터를 기반으로 학습
  - 과적합이 염려되는 경우 subsample을 1보다 **작게** 설정
