# **1. 분류(Classification)의 개요**
- 지도 학습: 라벨(label), 즉 명시적인 정답이 있는 데이터가 주어진 상태에서 학습하는 머신러닝 방식
- 분류는 ```지도 학습```의 일종
  - 주어진 데이터의 피처와 레이블 값(결정 값, 클래스 값)을 머신러닝 알고리즘으로 학습해 모델을 생성하고, 생성된 모델에 **새로운** 데이터 값이 주어졌을 때 미지의 ```레이블 값을 예측```하는 것
  - 기존 데이터가 어떤 레이블에 속하는 지 **패턴**을 알고리즘으로 인지한 뒤에 새롭게 관측된 데이터에 대한 ```레이블```을 판별하는 것
  
- 다양한 분류 알고리즘
  - ```나이브 베이즈(Naive Bayes)```: 베이즈(Bayes) 통계와 생성 모델에 기반
  - ```로지스틱 회귀(Logistic Regression)```: 독립변수와 종속변수의 **선형** 관계성에 기반
  - ```결정 트리(Decision Tree)```: 데이터 균일도에 따른 최대 분류 마진을 효과적으로 찾아줌
  - ```서포트 벡터 머신(Support Vector Machine)```: 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아줌
  - ```최소 근접(Nearest Neighbor) 알고리즘```: 근접 거리 기준
  - ```신경망(Neural Network)```: 심층 연결 기반
  - ```앙상블(Ensemble)```: 서로 다른(또는 같은) 머신러닝 알고리즘을 결합하는 방식
    - 대부분은 **같은** 알고리즘을 결합 
    - 매우 많은 여러 개의 **약한** 학습기(-> 예측 성능이 상대적으로 떨어지는 학습 알고리즘)를 결합해 확률적 보완과 오류가 발생한 부분에 대한 ```가중치를 계속 업데이트```하면서 예측 성능을 향상시키는 방식
    - 일반적으로 **배깅(Bagging)** 과 **부스팅(Boosting)** 방식으로 나뉨
    - ```랜덤 포레스트(Random Forest)```: 대표적인 배깅 알고리즘
    - ```그레디언트 부스팅(Gradient Boosting)```, ```XGBoost```, ```LightGBM```: 부스팅 알고리즘 

# **2. 결정 트리(Decision Tree)**
- 데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 **트리** 기반의 분류 규칙을 만드는 것
  - 주로 ```if/else``` 기반( -> 스무고개)
- **결정 트리의 구조**  

<img src = "https://user-images.githubusercontent.com/98953721/235280544-db6a034b-6ed7-459f-b7f1-84af5d267108.png" width = 500 height = 250>

- 데이터 세트에 피처가 있고 이러한 피처가 결합해 규칙 조건을 만들 때마다 ```규칙 노드```가 생성됨
  - 많은 규칙이 있으면 분류를 결정하는 방식이 복잡해짐 -> **과적합(Overfitting)** 위험
  - 트리의 깊이(depth)가 **깊어질수록** 결정 트리의 예측 성능이 **저하** 가능성이 높아짐
  - 가능한 한 **적은** 결정 노드로 높은 예측 정확도를 가지도록 트리 **분할(split)** 규착 찾기

- **균일한 데이터**  
  - 데이터 세트의 균일도: 데이터를 구분하는 데 필요한 정보의 양
  <img src = "https://user-images.githubusercontent.com/98953721/235281082-2c634c01-9427-4b7f-9e94-2ffaadbe54df.png" width = 600 height =200>
  
  - 결정 노드는 정보 균일도가 **높은** 데이터 세트를 먼저 선택할 수 있도록 규칙 조건을 생성
    - 정보 균일도가 데이터 세트로 쪼개질 수 있도록 조건을 찾아 ```서브 데이터 세트```를 만들고, 다시 이 서브 데이터 세트에서 균일도가 **높은** 자식 데이터 세트 쪼개는 방식을 자식 트리로 내려가면서 반복
  
  - 정보의 균일도 측정 방식
    - ```정보 이득(Information Gain)```
      - 엔트로피(주어진 데이터 집합의 혼잡도) 개념 기반
        - 서로 다른 값이 섞여있으면 **높은** 엔트로피 
      - 정보 이득 지수: 1 - (엔트로피 지수)
        - 정보 이득이 높은 속성을 기준으로 데이터 분할 규칙을 생성
        
    - ```지니 계수(Gini)```
      - 불평등 지수
      - 0이 가장 평등하고 1로 갈수록 불평등
      - 머신러닝 적용 시엔 지니 계수가 낮을수록 데이터 균일도가 **높은** 것으로 해석

- 사이킷런의 ```DecisionTreeClassifier```는 **지니 계수**를 이용해 데이터 세트를 분할
  - 데이터 세트를 분할하는 데 ```가장 좋은 조건(-> 정보 이득이 높음/ 지니 계수가 낮음)```을 찾아서 자식 트리 노드에 걸쳐 반복적으로 분할한 뒤, 데이터가 모두 특정 분류에 속하게 되면 분할을 멈추고 분류를 결정하는 방식

<img src = "https://user-images.githubusercontent.com/98953721/235281334-dbbaea27-7cc7-47fe-be1f-3f889bdff665.png" width = 700 height = 400>

### **2-1. 결정 트리 모델의 특징**
- 장점> **균일도** 기반 분류
  - 쉽고 직관적인 알고리즘, 시각화
  - 특별한 경우릎 제외하고는 각 피처의 스케일링과 정규화 등의 전처리 작업이 필요 x
- 단점> 과적합으로 인한 정확도 감소
  - 사전에 트리의 크기를 제한하는 튜닝 필요

### **2-2. 결정 트리 파라미터**














































