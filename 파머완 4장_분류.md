# **1. 분류(Classification)의 개요**
- 지도 학습: 라벨(label), 즉 명시적인 정답이 있는 데이터가 주어진 상태에서 학습하는 머신러닝 방식
- 분류는 ```지도 학습```의 일종
  - 주어진 데이터의 피처와 레이블 값(결정 값, 클래스 값)을 머신러닝 알고리즘으로 학습해 모델을 생성하고, 생성된 모델에 **새로운** 데이터 값이 주어졌을 때 미지의 ```레이블 값을 예측```하는 것
  - 기존 데이터가 어떤 레이블에 속하는 지 **패턴**을 알고리즘으로 인지한 뒤에 새롭게 관측된 데이터에 대한 ```레이블```을 판별하는 것
  
- 다양한 분류 알고리즘
  - ```나이브 베이즈(Naive Bayes)```: 베이즈(Bayes) 통계와 생성 모델에 기반
  - ```로지스틱 회귀(Logistic Regression)```: 독립변수와 종속변수의 **선형** 관계성에 기반
  - ```결정 트리(Decision Tree)```: 데이터 균일도에 따른 최대 분류 마진을 효과적으로 찾아줌
  - ```서포트 벡터 머신(Support Vector Machine)```: 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아줌
  - ```최소 근접(Nearest Neighbor) 알고리즘```: 근접 거리 기준
  - ```신경망(Neural Network)```: 심층 연결 기반
  - ```앙상블(Ensemble)```: 서로 다른(또는 같은) 머신러닝 알고리즘을 결합하는 방식
    - 대부분은 **같은** 알고리즘을 결합 
    - 매우 많은 여러 개의 **약한** 학습기(-> 예측 성능이 상대적으로 떨어지는 학습 알고리즘)를 결합해 확률적 보완과 오류가 발생한 부분에 대한 ```가중치를 계속 업데이트```하면서 예측 성능을 향상시키는 방식
    - 일반적으로 **배깅(Bagging)** 과 **부스팅(Boosting)** 방식으로 나뉨
    - ```랜덤 포레스트(Random Forest)```: 대표적인 배깅 알고리즘
    - ```그레디언트 부스팅(Gradient Boosting)```, ```XGBoost```, ```LightGBM```: 부스팅 알고리즘 

# **2. 결정 트리(Decision Tree)**
- 데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 **트리** 기반의 분류 규칙을 만드는 것
  - 주로 ```if/else``` 기반( -> 스무고개)
- **결정 트리의 구조**  

<img src = "https://user-images.githubusercontent.com/98953721/235280544-db6a034b-6ed7-459f-b7f1-84af5d267108.png" width = 500 height = 250>

- 데이터 세트에 피처가 있고 이러한 피처가 결합해 규칙 조건을 만들 때마다 ```규칙 노드```가 생성됨
  - 많은 규칙이 있으면 분류를 결정하는 방식이 복잡해짐 -> **과적합(Overfitting)** 위험
  - 트리의 깊이(depth)가 **깊어질수록** 결정 트리의 예측 성능이 **저하** 가능성이 높아짐
  - 가능한 한 **적은** 결정 노드로 높은 예측 정확도를 가지도록 트리 **분할(split)** 규착 찾기

- **균일한 데이터**  
  - 데이터 세트의 균일도: 데이터를 구분하는 데 필요한 정보의 양
  <img src = "https://user-images.githubusercontent.com/98953721/235281082-2c634c01-9427-4b7f-9e94-2ffaadbe54df.png" width = 600 height =200>
  
  - 결정 노드는 정보 균일도가 **높은** 데이터 세트를 먼저 선택할 수 있도록 규칙 조건을 생성
    - 정보 균일도가 데이터 세트로 쪼개질 수 있도록 조건을 찾아 ```서브 데이터 세트```를 만들고, 다시 이 서브 데이터 세트에서 균일도가 **높은** 자식 데이터 세트 쪼개는 방식을 자식 트리로 내려가면서 반복
  
  - 정보의 균일도 측정 방식
    - ```정보 이득(Information Gain)```
      - 엔트로피(주어진 데이터 집합의 혼잡도) 개념 기반
        - 서로 다른 값이 섞여있으면 **높은** 엔트로피 
      - 정보 이득 지수: 1 - (엔트로피 지수)
        - 정보 이득이 높은 속성을 기준으로 데이터 분할 규칙을 생성
        
    - ```지니 계수(Gini)```
      - 불평등 지수
      - 0이 가장 평등하고 1로 갈수록 불평등
      - 머신러닝 적용 시엔 지니 계수가 낮을수록 데이터 균일도가 **높은** 것으로 해석

- 사이킷런의 ```DecisionTreeClassifier```는 **지니 계수**를 이용해 데이터 세트를 분할
  - 데이터 세트를 분할하는 데 ```가장 좋은 조건(-> 정보 이득이 높음/ 지니 계수가 낮음)```을 찾아서 자식 트리 노드에 걸쳐 반복적으로 분할한 뒤, 데이터가 모두 특정 분류에 속하게 되면 분할을 멈추고 분류를 결정하는 방식

<img src = "https://user-images.githubusercontent.com/98953721/235281334-dbbaea27-7cc7-47fe-be1f-3f889bdff665.png" width = 700 height = 400>

### **2-1. 결정 트리 모델의 특징**
- 장점> **균일도** 기반 분류
  - 쉽고 직관적인 알고리즘, 시각화
  - 특별한 경우릎 제외하고는 각 피처의 스케일링과 정규화 등의 전처리 작업이 필요 x
- 단점> 과적합으로 인한 정확도 감소
  - 사전에 트리의 크기를 제한하는 튜닝 필요

### **2-2. 결정 트리 파라미터**
- 결정 트리 알고리즘
  - ```DecisionTreeClassifier```: 분류를 위한 클래스
  - ```DecisionTreeRegressor```: 회귀를 위한 클래스
- ```CART``` 알고리즘 기반 

- 결정 트리는 규칙 생성 로직을 미리 제어하지 않으면 완벽하게 클래스 값을 구별해내기 위해 트리 노드를 **계속**해서 만들어 나감
  - 결국 매우 복잡한 규칙 트리가 만들어져 모델이 쉽게 **과적합**되는 문제점을 가지게 됨
  - 과적합 위험이 상당히 높은 ML 알고리즘
  - 결정 트리 알고리즘을 제어하는 대부분의 하이퍼 파라미터는 복잡한 트리가 생성되는 것을 막기 위한 용도임
  
- **Parameters>**    
  - ```min_samples_split```
    - 노드를 분할하기 위한 최소한의 샘플 데이터 수 -> 과적합 제어
    - **default = 2**, 작게 설정할수록 분할되는 노드 수 증가 -> 과적합 가능성 증가
    - 과적합 제어, 1로 설정할 경우 분할되는 노드가 많아져서 과적합 가능성 증가 
    
    <img src = "https://user-images.githubusercontent.com/98953721/235286174-d382e3b8-3c90-4fd2-a248-4c3f9ccccea8.png" width = 700 height = 500>
    
  - ```min_samples_leaf```
    - 말단 노드(leaf)가 되기 위한 최소한의 샘플 데이터 수
      - 노드 생성 규칙이 변경될 수 있음
    - 과적합 제어 용도
    - 비대칭적(imbalanced) 데이터의 경우 특정 클래스의 데이터가 극도로 작아지는 문제가 발생할 수 있음
    <img src = "https://user-images.githubusercontent.com/98953721/235286277-2455abd5-074c-4444-954b-6ece34984f89.png" width = 700 height = 500>
 
  - ```max_features```
    - 최적의 분할을 위해 고려할 최대 피처 개수, default = None
    - ```int```형으로 지정하면 대상 피처의 **개수**, ```float```로 지정하면 전체 피처 중 대상 피처의 **퍼센트**
    - ```sqrt```: 전체 피처 중 sqrt(전체 피처 개수)
    - ```auto```: sqrt와 동일
    - ```log```: 전체 피처 중 log2(전체 피처 개수)
    - ```None```: 데이터 세트의 **모든** 피처를 사용
    
  - ```max_depth```
    - 트리의 최대 깊이 규정
    - default: None -> 완벽하게 클래스 결정 값이 될 때까지 깊이를 계속 키우며 분할하거나 노드가 가지는 데이터 수가 min_samples_split보다 작아질 때까지 계속 깊이를 증가시킴
    - 깊이가 깊어지면 최대 분할로 인한 과적합이 발생할 수 있음 -> 과적합 가능성, 적절한 제어 필요
    
    <img src = "https://user-images.githubusercontent.com/98953721/235285955-2efc5588-8eea-4f81-ab98-3d36d3f697ae.png" width = 700 height = 400>
    
  - ```max_leaf_nodes```
    - 말단 노드(leaf)의 최대 개수

### **2-3. 결정 트리 모델의 시각화**
- **Graphviz** 패키지 활용
  - 사이킷런의 ```export_graphviz()``` 함수 인자로 학습이 완료된 estimator, 피처 이름 리스트, 레이블 이름 리스트 입력 시 학습된 결정 트리 규칙을 실제 트리 형태로 시각화
  - Graphviz가 읽어 들여서 그래프 형태로 시각화 할 수 있는 출력 파일 생성
  - 출력 결과를 통해 각 규칙에 따른 트리의 브랜치(branch) 노드와 말단 리프(leaf) 노드가 어떻게 구성되는지 파악 가능
  
- **결과 해석**    
  - ```petal_length(cm) <= 2.45```와 같이 피처의 조건이 있는 것은 자식 노드를 만들기 위한 규칙 조건
  - 조건이 없는 경우 **리프 노드**
  - ```gini```: 다음의 value = [ ]로 주어진 데이터 분포에서의 지니 계수
  - ```samples```: 현 규칙에 해당하는 데이터 건수
  - ```value = [ ]```: 클래스 값 기반의 데이터 건수

### **2-4. 규칙 조건 생성 과정**
- 결정 트리는 **균일도**에 기반해 어떠한 속성을 규칙 조건으로 선택하느냐가 중요함
  - 중요한 **몇 개**의 피처가 명확한 규칙 트리를 만드는 데 크게 기여
  - 모델을 좀 더 간결하고 이상치(outlier)에 강한 모델을 만들 수 있음
  - DecisionTreeClassifier 객체의 ```feature_importances_``` 속성으로 확인 가능
  
- ```feature_importances_``` 속성
  - ndarray 형태로 값을 반환, 피처 순서대로 값이 할당됨
  - 값이 **높을수록** 피처의 중요도가 높음

### **2-5. 결정 트리 과적합(Overfitting)**
<img src = "https://user-images.githubusercontent.com/98953721/235287022-cf69736e-b1f7-4415-86fa-c8bc315afe9e.png" width = 400 height = 250>

- 일부 이상치(Outlier) 데이터까지 분류하기 위해 분할이 자주 일어남 
  - 결정 기준 경계가 매우 **많아진** 것을 확인할 수 있음
- 결정 트리의 **기본 파라미터** 설정은 리프 노드 안의 데이터가 모두 균일하거나 하나만 존재해야 하는 엄격한 분할 기준을 가짐 -> 결정 기준 경계가 많아짐, 복잡해짐
- 복잡한 모델의 경우 학습 데이터 세트의 특성과 약간만 다른 형태의 데이터 세트예측 시 예측 정확도가 **떨어지는** 문제 발생

---
<img src = "https://user-images.githubusercontent.com/98953721/235287103-fa5d657d-7240-400c-a113-ded901f07549.png" width = 400 height = 250>

- 이상치에 크게 반응하지 않으면서 좀 더 일반화된 분류 규칙에 따라 분류되었음을 확인할 수 있음




































