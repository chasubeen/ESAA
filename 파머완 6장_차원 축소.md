# **1. 차원 축소(Dimension Reduction)**
### **1-1. 차원 축소**
- 매우 많은 피처로 구성된 다차원 데이터 세트의 차원을 축소해 새로운 차원의 데이터 세트의 차원을 축소해 새로운 차원의 데이터 세트를 생성하는 것
- 일반적으로 차원이 증가할수록 데이터 포인트 간의 거리가 기하급수적으로 멀어지게 되고, **희소(sparse)** 한 구조를 가지게 됨
  - 수백 개 이상의 피처로 구성된 데이터 세트의 경우 상대적으로 적은 차원에서 학습된 모델보다 예측 신뢰도가 떨어짐
- 피처가 많을 경우 개별 피처 간에 상관관계가 **높을** 가능성이 큼
  - 선형 회귀 등 **선형** 모델에서는 입력 변수 간의 상관관계가 높을 경우 이로 인한 다중공선성 문제로 모델의 예측 성능이 저하됨  
- 효과
  - 매우 많은 다차원의 피처를 차원 축소해 피처 수를 줄이면 더 **직관적**으로 데이터를 해석할 수 있음
  - 차원 축소 시 학습 데이터의 크기가 줄어들어서 학습에 필요한 처리 능력을 줄일 수 있음

### **1-2. 차원 축소의 종류**
#### **a) 피처 선택(feature selection)**  
- 특정 피처에 종속성이 강한 불필요한 피처는 아예 제거하고, 데이터의 특징을 잘 나타내는 주요 피처만 선택하는 것 
#### **b) 피처 추출(feature extraction)**  
- 기존 피처를 저차원의 중요 피처로 압축해서 추출하는 것
  - 피처를 함축적으로 더 잘 설명할 수 있는 또 다른 공간으로 매핑해 추출하는 것
  - 기존 피처가 전혀 인지하기 어려웠던 **잠재적인 요소** (Latent Factor)를 추출하는 것 
- 새롭게 추출된 중요 특성은 기존의 피처가 압축된 것이므로 기존의 피처와는 완전히 다른 값

### **1-3. 차원 축소 활용**
#### **a) 이미지 데이터**
- 잠재된 특성을 피처로 도출해 함축적 형태의 이미지 변환과 압축을 수행할 수 있음
- 변환된 이미지는 원본 이미지보다 훨씬 **적은** 차원
  - 이미지 분류 등의 분류 수행 시 과적합 영향력이 작아져서 오히려 원본 데이터로 예측하는 것보다 예측 성능을 더 끌어 올릴 수 있음 
#### **b) 텍스트 분석**
- 텍스트 문서의 숨겨진 의미 추출
- 문서 내 단어들의 구성에서 숨겨져 있는 시맨틱(semantic) 의미나 토픽(topic)을 잠재 요소로 간주하고 이를 찾아낼 수 있도록 함
- ```SVD```, ```NMF``` 등이 존재함


# **2. PCA(Principal Component Analysis)**
### **2-1. PCA 개요**
- 가장 대표적인 차원 축소 기법
- 여러 변수 간에 존재하는 상관관계를 이용해 이를 대표하는 **주성분** 을 추출해 차원을 축소하는 기법
  - 기존 데이터의 정보 유실이 최소화되는 방향으로 축소
- PCA는 가장 **높은** 분산을 가지는 데이터의 축을 찾아 해당 축으로 차원을 축소
  - 분산이 데이터의 특성을 가장 잘 나타내는 것으로 간주
  - 이것을 PCA의 **주성분** 이라 함   
- 원본 데이터의 피처 개수에 비해 매우 작은 주성분으로 원본 데이터의 총 변동성을 대부분 설명할 수 있는 분석법
- PCA는 속성의 스케일에 영향을 받음
  - 여러 속성을 PCA로 압축하기 전에 각 속성값을 **동일한** 스케일로 변환하는 과정이 필요 

<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/e0b89c02-52b5-4ae9-b85e-6df350175e6c" width = 500 height = 150>

- ```진행 과정```
  1. 가장 큰 데이터 변동성(variance)을 기반으로 첫 번째 벡터 축을 생성
  2. 두 번째 축은 해당 벡터 축에 직각이 되는 벡터(직교 벡터)를 축으로 함
  3. 세 번째 축은 다시 두 번째 축과 직각이 되는 벡터를 설정하는 방식으로 축을 생성
  4. 생성된 벡터 축에 원본 데이터를 투영 -> 벡터 축의 개수만큼의 차원으로 원본 데이터가 차원 축소됨

- PCA 객체의 ```explained_variance_ratio_``` 속성을 통해 전체 변동성에서 개별 PCA 컴포넌트별로 차지하는 변동성의 비율을 파악할 수 있음


### **2-2. 선형대수 관점에서의 해석**
- 입력 데이터의 **공분산 행렬** (covariance matrix)을 고윳값 분해하고, 이렇게 구한 고유벡터에 입력 데이터를 선형 변환하는 것
  - 해당 고유벡터가 PCA의 주성분 벡터 -> 입력 데이터의 분산이 큰 방향을 나타냄
  - 고윳값(eigenvalue) = 고유벡터의 크기 = 입력 데이터의 분산  
 
(※ 자세한 내용은 p.380 ~ 381 참고하기)  

- 입력 데이터의 공분산 행렬이 고유벡터와 고유값으로 분해될 수 있으며, 이렇게 분해된 고유벡터를 이용해 입력 데이터를 **선형 변환** 하는 방식
- ```PCA 진행 과정(선형 대수적 관점)```
  1. 입력 데이터 세트의 공분산 행렬을 생성
  2. 공분산 행렬의 고유벡터와 고유값을 계산
  3. 고유값이 가장 큰 순으로 K개(PCA 변환 차수)만큼 고유벡터를 추출
  4. 고유값이 가장 큰 순으로 추출된 고유벡터를 이용해 새롭게 입력 데이터를 변환
  
  
# **3. LDA(Linear Discriminant Analysis)**

### **3-1. LDA 개요**
- 선형 판별 분석법
  - PCA와 유사하게 입력 데이터 세트를 저차원 공간에 투영해 차원을 축소하는 기법
  - LDA는 지도학습의 분류에 사용하기 쉽도록 개별 클래스를 분별할 수 있는 기준을 최대한 유지하면서 차원을 축소
    - PCA: 입력 데이터의 변동성이 가장 **큰** 축을 선택
    - LDA: 입력 데이터의 결정 값 클래스를 **최대한으로 분리** 할 수 있는 축을 선택
  - LDA는 PCA와 다르게 차원 축소 시 클래스의 **결정값** 을 필요로 함
  
- LDA는 특정 공간상에서 클래스 분리를 최대화하는 축을 찾기 위해 ```클래스 간 분산```(between-class scatter)과 ```클래스 내부 분산```(within-ckass scatter)의 비율을 최대화하는 방식으로 차원 축소
  - 클래스 간 분산은 최대한 크게, 클래스 내부의 분산은 최대한 작게
  - 클래스 간 분산과 클래스 내부 분산 행렬을 생성한 뒤, 이 행렬에 기반해 고유벡터를 구하고 입력 데이터를 투영함
  

<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/8b638691-6f16-4344-93c9-08adccac9768" width = 300 height = 200>

<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/c9963588-18ec-40e4-8cdc-6ef0e1121fce" width = 700 height = 400>

- 사이킷런은 LDA를 ```LinearDiscriminantAnalysis``` 클래스로 제공함


# **4. SVD(Singular Value Decomposition)**

### **4-1. SVD 개요**
- pca






















































