# **1. 차원 축소(Dimension Reduction)**
### **1-1. 차원 축소**
- 매우 많은 피처로 구성된 다차원 데이터 세트의 차원을 축소해 새로운 차원의 데이터 세트의 차원을 축소해 새로운 차원의 데이터 세트를 생성하는 것
- 일반적으로 차원이 증가할수록 데이터 포인트 간의 거리가 기하급수적으로 멀어지게 되고, **희소(sparse)** 한 구조를 가지게 됨
  - 수백 개 이상의 피처로 구성된 데이터 세트의 경우 상대적으로 적은 차원에서 학습된 모델보다 예측 신뢰도가 떨어짐
- 피처가 많을 경우 개별 피처 간에 상관관계가 **높을** 가능성이 큼
  - 선형 회귀 등 **선형** 모델에서는 입력 변수 간의 상관관계가 높을 경우 이로 인한 다중공선성 문제로 모델의 예측 성능이 저하됨  
- 효과
  - 매우 많은 다차원의 피처를 차원 축소해 피처 수를 줄이면 더 **직관적**으로 데이터를 해석할 수 있음
  - 차원 축소 시 학습 데이터의 크기가 줄어들어서 학습에 필요한 처리 능력을 줄일 수 있음

### **1-2. 차원 축소의 종류**
#### **a) 피처 선택(feature selection)**  
- 특정 피처에 종속성이 강한 불필요한 피처는 아예 제거하고, 데이터의 특징을 잘 나타내는 주요 피처만 선택하는 것 
#### **b) 피처 추출(feature extraction)**  
- 기존 피처를 저차원의 중요 피처로 압축해서 추출하는 것
  - 피처를 함축적으로 더 잘 설명할 수 있는 또 다른 공간으로 매핑해 추출하는 것
  - 기존 피처가 전혀 인지하기 어려웠던 **잠재적인 요소** (Latent Factor)를 추출하는 것 
- 새롭게 추출된 중요 특성은 기존의 피처가 압축된 것이므로 기존의 피처와는 완전히 다른 값

### **1-3. 차원 축소 활용**
#### **a) 이미지 데이터**
- 잠재된 특성을 피처로 도출해 함축적 형태의 이미지 변환과 압축을 수행할 수 있음
- 변환된 이미지는 원본 이미지보다 훨씬 **적은** 차원
  - 이미지 분류 등의 분류 수행 시 과적합 영향력이 작아져서 오히려 원본 데이터로 예측하는 것보다 예측 성능을 더 끌어 올릴 수 있음 
#### **b) 텍스트 분석**
- 텍스트 문서의 숨겨진 의미 추출
- 문서 내 단어들의 구성에서 숨겨져 있는 시맨틱(semantic) 의미나 토픽(topic)을 잠재 요소로 간주하고 이를 찾아낼 수 있도록 함
- ```SVD```, ```NMF``` 등이 존재함


# **2. PCA(Principal Component Analysis)**
### **2-1. PCA 개요**
- 가장 대표적인 차원 축소 기법
- 여러 변수 간에 존재하는 상관관계를 이용해 이를 대표하는 **주성분** 을 추출해 차원을 축소하는 기법
  - 기존 데이터의 정보 유실이 최소화되는 방향으로 축소
- PCA는 가장 **높은** 분산을 가지는 데이터의 축을 찾아 해당 축으로 차원을 축소
  - 분산이 데이터의 특성을 가장 잘 나타내는 것으로 간주
  - 이것을 PCA의 **주성분** 이라 함   
- 원본 데이터의 피처 개수에 비해 매우 작은 주성분으로 원본 데이터의 총 변동성을 대부분 설명할 수 있는 분석법
- PCA는 속성의 스케일에 영향을 받음
  - 여러 속성을 PCA로 압축하기 전에 각 속성값을 **동일한** 스케일로 변환하는 과정이 필요 

<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/e0b89c02-52b5-4ae9-b85e-6df350175e6c" width = 500 height = 150>

- ```진행 과정```
  1. 가장 큰 데이터 변동성(variance)을 기반으로 첫 번째 벡터 축을 생성
  2. 두 번째 축은 해당 벡터 축에 직각이 되는 벡터(직교 벡터)를 축으로 함
  3. 세 번째 축은 다시 두 번째 축과 직각이 되는 벡터를 설정하는 방식으로 축을 생성
  4. 생성된 벡터 축에 원본 데이터를 투영 -> 벡터 축의 개수만큼의 차원으로 원본 데이터가 차원 축소됨

- PCA 객체의 ```explained_variance_ratio_``` 속성을 통해 전체 변동성에서 개별 PCA 컴포넌트별로 차지하는 변동성의 비율을 파악할 수 있음


### **2-2. 선형대수 관점에서의 해석**
- 입력 데이터의 **공분산 행렬** (covariance matrix)을 고윳값 분해하고, 이렇게 구한 고유벡터에 입력 데이터를 선형 변환하는 것
  - 해당 고유벡터가 PCA의 주성분 벡터 -> 입력 데이터의 분산이 큰 방향을 나타냄
  - 고윳값(eigenvalue) = 고유벡터의 크기 = 입력 데이터의 분산  
 
(※ 자세한 내용은 p.380 ~ 381 참고하기)  

- 입력 데이터의 공분산 행렬이 고유벡터와 고유값으로 분해될 수 있으며, 이렇게 분해된 고유벡터를 이용해 입력 데이터를 **선형 변환** 하는 방식
- ```PCA 진행 과정(선형 대수적 관점)```
  1. 입력 데이터 세트의 공분산 행렬을 생성
  2. 공분산 행렬의 고유벡터와 고유값을 계산
  3. 고유값이 가장 큰 순으로 K개(PCA 변환 차수)만큼 고유벡터를 추출
  4. 고유값이 가장 큰 순으로 추출된 고유벡터를 이용해 새롭게 입력 데이터를 변환
  
  
# **3. LDA(Linear Discriminant Analysis)**

### **3-1. LDA 개요**
- 선형 판별 분석법
  - PCA와 유사하게 입력 데이터 세트를 저차원 공간에 투영해 차원을 축소하는 기법
  - LDA는 지도학습의 분류에 사용하기 쉽도록 개별 클래스를 분별할 수 있는 기준을 최대한 유지하면서 차원을 축소
    - PCA: 입력 데이터의 변동성이 가장 **큰** 축을 선택
    - LDA: 입력 데이터의 결정 값 클래스를 **최대한으로 분리** 할 수 있는 축을 선택
  - LDA는 PCA와 다르게 차원 축소 시 클래스의 **결정값** 을 필요로 함
  
- LDA는 특정 공간상에서 클래스 분리를 최대화하는 축을 찾기 위해 ```클래스 간 분산```(between-class scatter)과 ```클래스 내부 분산```(within-ckass scatter)의 비율을 최대화하는 방식으로 차원 축소
  - 클래스 간 분산은 최대한 크게, 클래스 내부의 분산은 최대한 작게
  - 클래스 간 분산과 클래스 내부 분산 행렬을 생성한 뒤, 이 행렬에 기반해 고유벡터를 구하고 입력 데이터를 투영함
  

<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/8b638691-6f16-4344-93c9-08adccac9768" width = 300 height = 200>

<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/c9963588-18ec-40e4-8cdc-6ef0e1121fce" width = 700 height = 400>

- 사이킷런은 LDA를 ```LinearDiscriminantAnalysis``` 클래스로 제공함


# **4. SVD(Singular Value Decomposition)**

### **4-1. SVD 개요**
- PCA와 유사한 행렬 분해 기법을 활용
  - 그러나 PCA와는 다르게 정방행렬뿐만 아니라 행과 열의 크기가 다른 행렬에도 적용 가능
- 다음과 같은 방식으로 m x n 크기의 행렬을 분해

<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/08529d9d-254a-4fe0-a84c-a275e4cf38e9" width = 150 height = 50>

- SVD는 **특이값 분해**로 불림
  - 행렬 U와 V에 속한 벡터는 특이벡터(singular vector)
  - 모든 특이벡터는 서로 **직교**하는 성질을 가짐
  - $\sum$ : 대각행렬, 행렬의 대각에 위치한 값을 제외한 나머지 위치의 값이 모두 0인 행렬
    - 0이 아닌 값이 행렬 A의 특이값 

- 일반적으로는 다음과 같이 $\sum$ 의 비대각인 부분과 대각원소 중에 특이값이 0인 부분도 모두 제거하고 제거된 $\sum$ 에 대응되는 U와 V 원소도 함께 제거해 차원을 줄인 형태로 SVD를 적용

<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/87eb9dd4-7297-47f3-9425-7370a2629106" width = 500 height = 200>

- 넘파이의 ```svd``` 모듈 사용
```python
from numpy.linalg import svd 
```

- ```svd()``` 함수를 활용해 U, Sigma, Vt를 도출할 수 있음

**Truncated SVD**  
- $\sum$의 대각원소 중에 상위 몇 개만 추출해서 여기에 대응하는 U와 V의 원소도 함께 제거해 더욱 차원을 줄인 형태로 분해하는 것
- 인위적으로 더 작은 차원의 행렬들로 분해하기에 원본 행렬로 정확하게 다시 복원할 수는 없음
  - 하지만 데이터 정보가 압축되어 분해됨에도 불구하고 상당한 수준으로 원본 행렬을 근사할 수는 있음 
- 사이파이 모듈에서만 가능
```python
from scipy.sparse.linalg import svds
``` 

cf) 사이킷런 TruncatedSVD 클래스를 이용한 변환  
- 사이파이의 svds와 같이 Truncated SVD 연산을 수행하여 원본 행렬을 분해하지만, 분해한 행렬을 반환하지는 않음
- PCA 클래스와 유사하게 ```fit()```과 ```transform()```을 호출해 원본 데이터를 몇 개의 주요 컴포넌트로 차원 축소해 변환
  - 원본 데이터를 Truncated SVD 방식으로 분해된 U * Sigma 행렬에 선형 변환해 생성  
- TruncatedSVD 변환 역시 PCA와 유사하게 변환 후에 품종별로 어느 정도 클러스터링이 가능할 정도로 각 변환 속성으로 뛰어난 고유성을 가지고 있음


# **5. NMF(Non-Negative Matrix Factorization)**

### **5-1. NMF 개요**
- Truncated SVD와 같이 낮은 랭크를 통한 행렬 근사(Low-Rank Approximation) 방식의 변형임
- NMF는 원본 행렬 내의 모든 원소 값이 모두 **양수**라는 것이 보장되면 다음과 같이 좀 더 간단하게 두 개의 기반 양수 행렬로 분해될 수 있는 기법을 지칭함

<img src = "https://github.com/chasubeen/ESAA_8th_YB/assets/98953721/29c1033f-67bd-4314-b0b1-888cfafe0768" width = 500 height = 150>

- 분해된 행렬은 잠재 요소를 특성으로 가지게 됨
  - 분해 행렬 W는 원본 행에 대해서 해당 잠재 요소의 값이 얼마나 되는지에 대응하며, 분해 행렬 H는 해당 잠재 요소가 원본 열(= 원본 속성)로 어떻게 구성됐는지를 나타내는 행렬

 - SVD와 유사하게 차원 축소를 통한 잠재 요소 도출로 이미지 변환 및 압축, 텍스트의 토픽 도출 등의 영역에서 이용되고 있음
- 사이킷런의 NMF 클래스를 통해 지원됨















































